{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Virtualization Repository","text":"<p>This site collects reference deployments for hypervisors and container platforms used in Vesta Lab projects.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Proxmox VE 8.4.1 \u2013 full bare-metal cluster walkthrough  </li> <li>VMware ESXi \u2013 work in progress </li> <li>Docker / Portainer \u2013 work in progress</li> <li>KVM / QEMU / Libvirt \u2013 work in progress</li> </ul>"},{"location":"docker/","title":"Overview","text":"<p>Work in progress</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/","title":"Proxmox VE 8.4.1 \u2014 Ceph Storage","text":"<p>All commands can be issued from any node. Replace device names as needed.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-1-install-ceph-on-every-node","title":"Step 1: Install Ceph on Every Node","text":"<p>Install the Ceph packages using the no-subscription repository:</p> <pre><code>pveceph install --repository no-subscription\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-2-initialize-the-ceph-cluster","title":"Step 2: Initialize the Ceph Cluster","text":"<p>Run once on one node (e.g., ve1):</p> <pre><code>pveceph init --network 10.30.0.0/24\n</code></pre> <p>Then create the Monitor and Manager daemons on each node:</p> <pre><code>pveceph mon create\npveceph mgr create\n</code></pre> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-3-create-osds","title":"Step 3: Create OSDs","text":"<p>Prepare and assign disks on each node.</p> Node Disk Class ve1 /dev/sda ssd ve2 /dev/vme0n1 ssd ve3 /dev/sda hdd <p>For each disk:</p> <pre><code>sgdisk --zap-all /dev/sda\nwipefs -a /dev/sda\npveceph osd create /dev/sda --crush-device-class ssd\n</code></pre> <p>Repeat for each disk across all nodes, changing the device accordingly.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-4-create-pools","title":"Step 4: Create Pools","text":"<p>Create different pools for VM storage and backups:</p> <pre><code>pveceph pool create ceph-pool --application rbd\nceph osd pool set ceph-pool size 2\n</code></pre> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-5-add-cephfs-for-iso-and-templates","title":"Step 5: Add CephFS for ISO and Templates","text":"<p>Create a CephFS volume and add it to Proxmox storage:</p> <pre><code>pveceph fs create cephfs --data-pool cephfs_data --metadata-pool cephfs_metadata\n</code></pre> <p>Then add it to Proxmox using:</p> <pre><code>pvesm add ceph-fs cephfs-store   --monhost 10.30.0.2 10.30.0.3 10.30.0.4   --content iso,vztmpl,backup   --mountpoint /mnt/pve/ceph-fs\n</code></pre> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-6-validate-the-setup","title":"Step 6: Validate the Setup","text":"<p>Check the Ceph cluster health:</p> <pre><code>ceph -s\npvesm status\n</code></pre> <p>You should see <code>HEALTH_OK</code> and the <code>cephfs-store</code> mounted correctly.</p> <p>Ceph is now configured and integrated with Proxmox VE.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/","title":"Proxmox VE 8.4.1 \u2014 Cluster Setup","text":"<p>Ensure that networking and <code>/etc/hosts</code> are configured correctly on all nodes before proceeding.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-1-create-the-cluster-on-ve1","title":"Step 1: Create the Cluster on ve1","text":"<p>Run the following command on ve1 to initialize the cluster:</p> <pre><code>pvecm create vesta-lab --link0 address=10.10.0.11,priority=1\n</code></pre> <p>This command sets up the cluster named <code>vesta-lab</code> and defines the primary communication interface (link0) using the management network.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-2-join-ve2-and-ve3-to-the-cluster","title":"Step 2: Join ve2 and ve3 to the Cluster","text":"<p>On ve2 and ve3, run:</p> <pre><code>pvecm add 10.10.0.11 --nodeid &lt;ID&gt;\n</code></pre> <p>Replace <code>&lt;ID&gt;</code> with a unique node ID (e.g., 2 for ve2, 3 for ve3). Make sure that node IDs are not duplicated.</p> <p>If you mistakenly created a local cluster on ve2/ve3, reset it before joining:</p> <pre><code>systemctl stop corosync pve-cluster\nrm -rf /etc/corosync/* /var/lib/pve-cluster/*\nsystemctl start pve-cluster\n</code></pre> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-3-regenerate-certificates-optional","title":"Step 3: Regenerate Certificates (Optional)","text":"<p>After all nodes are joined and accessible, regenerate certificates from ve1 to ensure consistency:</p> <pre><code>pvecm updatecerts --force\nsystemctl restart pveproxy pvedaemon\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-4-validate-cluster-status","title":"Step 4: Validate Cluster Status","text":"<p>On any node, run:</p> <pre><code>pvecm status\n</code></pre> <p>Expected output:</p> <pre><code>Quorate: Yes\nNodes: 3\n</code></pre> <p>You should see a <code>Quorate: Yes</code> message indicating that quorum is established. If not, check the time sync (<code>timedatectl</code>), hostnames, and firewall settings.</p> <p></p> <p>Cluster is now initialized and operational.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/","title":"High Availability (HA) Setup in Proxmox VE","text":"<p>This guide explains how to configure HA in a 3-node Proxmox cluster to ensure critical VMs are automatically restarted on other nodes in case of failure.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#requirements","title":"Requirements","text":"<ul> <li>A Proxmox VE cluster with at least 3 nodes.</li> <li>Shared or distributed storage (e.g., Ceph RBD or CephFS).</li> <li>Correct time synchronization across all nodes (<code>timedatectl</code>).</li> <li>Fencing or watchdog device recommended for production.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-1-check-cluster-status","title":"Step 1: Check Cluster Status","text":"<p>Ensure all nodes are online and the cluster is quorate:</p> <pre><code>pvecm status\n</code></pre> <p>Expected output should include <code>Quorate: Yes</code>.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-2-enable-ha-manager","title":"Step 2: Enable HA Manager","text":"<p>Check the HA manager service:</p> <pre><code>systemctl status pve-ha-lrm\nsystemctl status pve-ha-crm\n</code></pre> <p>If not running, enable them:</p> <pre><code>systemctl enable --now pve-ha-lrm\nsystemctl enable --now pve-ha-crm\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-3-mark-vms-as-ha-managed","title":"Step 3: Mark VMs as HA-Managed","text":"<p>Use the web GUI or CLI to add a VM to HA:</p> <pre><code>ha-manager add vm:100\n</code></pre> <p>Replace <code>100</code> with the ID of your critical VM.</p> <p>To list HA-managed services:</p> <pre><code>ha-manager status\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-4-simulate-node-failure","title":"Step 4: Simulate Node Failure","text":"<p>You can test HA by rebooting a node or stopping <code>pve-cluster</code>:</p> <pre><code>systemctl stop pve-cluster\n</code></pre> <p>Check if the HA service automatically starts the VM on another node.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#notes","title":"Notes","text":"<ul> <li>HA requires shared storage to allow VMs to be moved or restarted on another node.</li> <li>Ceph RBD is ideal for HA setups.</li> <li>Always test HA scenarios before going into production.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#screenshots-diagrams","title":"Screenshots / Diagrams","text":""},{"location":"proxmox/8.4.1/bare-metal/docs/installation/","title":"Installing the Proxmox Nodes","text":"Node mgmt (VLAN 10) storage/cluster (VLAN 30) Hardware (example) ve1 10.10.0.11 10.30.0.2 NVMe 250\u202fGB + NVMe 480\u202fGB ve2 10.10.0.12 10.30.0.3 SATA 240\u202fGB + SSD 120\u202fGB ve3 10.10.0.13 10.30.0.4 NVMe 500\u202fGB + HDD 1\u202fTB <p>VLAN 20 (services) and VLAN 40 (backups) exist but are not required during installation.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-1-install-proxmox-ve-8","title":"Step 1: Install Proxmox VE 8","text":"<ul> <li>Download the latest Proxmox VE ISO from the official site.</li> <li>Boot each host using the ISO.</li> <li>Choose ZFS (single disk) for lab setups or ext4/LVM-thin depending on your goals.</li> <li>Assign the management IP (VLAN 10) during the installer process.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-2-update-and-configure-apt-repositories","title":"Step 2: Update and Configure APT Repositories","text":"<p>After the first boot, run:</p> <pre><code>echo \"deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription\" &gt; /etc/apt/sources.list.d/pve-no-sub.list\napt update &amp;&amp; apt full-upgrade -y\n</code></pre> <p>Disable the enterprise repository if it's present:</p> <pre><code>sed -i 's/^deb/#deb/' /etc/apt/sources.list.d/pve-enterprise.list\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-3-set-time-sync-ntp","title":"Step 3: Set Time Sync (NTP)","text":"<p>Ensure all nodes have time synchronized:</p> <pre><code>timedatectl set-ntp true\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-4-configure-hosts-file","title":"Step 4: Configure Hosts File","text":"<p>Edit <code>/etc/hosts</code> on all nodes to include:</p> <pre><code>10.10.0.11 ve1.vestasec.com ve1\n10.10.0.12 ve2.vestasec.com ve2\n10.10.0.13 ve3.vestasec.com ve3\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-5-reboot-the-node","title":"Step 5: Reboot the Node","text":"<p>Once all settings are applied, reboot each node:</p> <pre><code>reboot\n</code></pre> <p>After reboot, verify connectivity using <code>ping</code> and ensure DNS resolution between nodes is working correctly.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/","title":"Proxmox VE 8.4.1 \u2014 Network and VLAN Configuration","text":"<p>This guide documents how to configure bridge interfaces and VLANs in Proxmox VE for clustered and virtualized environments.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#vlan-architecture","title":"VLAN Architecture","text":"VLAN ID Name Purpose CIDR 10 mgmt Management and Web GUI 10.10.0.0/24 20 services Container/VM services 10.20.0.0/24 30 cluster Ceph / Corosync traffic 10.30.0.0/24 40 iot IoT devices (optional) 10.40.0.0/24 50 guest Guest access VLAN 10.50.0.0/24"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#vmbr0-main-vlan-aware-bridge","title":"vmbr0 - Main VLAN-aware Bridge","text":"<p>In <code>/etc/network/interfaces</code>:</p> <pre><code>auto lo\niface lo inet loopback\n\nauto eno1\niface eno1 inet manual\n\nauto vmbr0\niface vmbr0 inet manual\n        bridge-ports eno1\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n</code></pre> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#vlan-tagged-subinterfaces","title":"VLAN-tagged Subinterfaces","text":"<p>Replace <code>10.30.0.11</code> with the node-specific management IP (e.g., ve1: <code>.11</code>, ve2: <code>.12</code>).</p> <pre><code>auto vmbr0.10\niface vmbr0.10 inet static\n        address 10.30.0.11/24\n        gateway 10.30.0.1\n        dns-servers 10.0.0.102 1.1.1.1\n        vlan-raw-device vmbr0\n</code></pre> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#notes","title":"Notes","text":"<ul> <li><code>bridge-vlan-aware yes</code> is preferred for modern setups.</li> <li>Ensure all trunk ports on the physical switch pass the required VLANs.</li> <li>For Ceph and Corosync, use VLAN 30 with MTU 1500 or 9000 (depending on hardware).</li> <li>Verify connectivity with <code>ping</code> and <code>ip link</code>.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#screenshots","title":"Screenshots","text":""},{"location":"proxmox/8.4.1/bare-metal/templates/","title":"Creating Cloud-Init VM Templates in Proxmox","text":"<p>This guide explains how to create a reusable Debian 12 template with cloud-init support in Proxmox.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#steps","title":"Steps","text":"<ol> <li>Download the Debian 12 generic cloud image (QCOW2).</li> <li>Create a new VM with ID 9000 and attach the downloaded disk.</li> <li>Configure boot settings and add a cloud-init drive.</li> <li>Mark the VM as a template.</li> </ol>"},{"location":"proxmox/8.4.1/bare-metal/templates/#script","title":"Script","text":"<p>The <code>create_debian12_template.sh</code> script automates this process:</p> <pre><code>./scripts/create_debian12_template.sh\n</code></pre> <p>Ensure the script has executable permissions and is run from the <code>proxmox/</code> directory.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#output","title":"Output","text":"<p>Once executed, you\u2019ll have a reusable cloud-init template with VM ID 9000 stored in your local Proxmox node.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#creating-cloud-init-vm-templates-in-proxmox_1","title":"Creating Cloud-Init VM Templates in Proxmox","text":"<p>This guide explains how to create a reusable Debian 12 template with cloud-init support in Proxmox.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#steps_1","title":"Steps","text":"<ol> <li>Download the Debian 12 generic cloud image (QCOW2).</li> <li>Create a new VM with ID 9000 and attach the downloaded disk.</li> <li>Configure boot settings and add a cloud-init drive.</li> <li>Mark the VM as a template.</li> </ol>"},{"location":"proxmox/8.4.1/bare-metal/templates/#script_1","title":"Script","text":"<p>The <code>create_debian12_template.sh</code> script automates this process:</p> <pre><code>./scripts/create_debian12_template.sh\n</code></pre> <p>Ensure the script has executable permissions and is run from the <code>proxmox/</code> directory.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#output_1","title":"Output","text":"<p>Once executed, you\u2019ll have a reusable cloud-init template with VM ID 9000 stored in your local Proxmox node.</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph/","title":"Ceph Troubleshooting","text":"Symptom Fix <code>401 Unauthorized</code> when installing Ceph Use the no-subscription repo:<code>echo 'deb http://download.proxmox.com/debian/ceph-quincy bookworm no-subscription' &gt; /etc/apt/sources.list.d/ceph.list</code> <code>device is already in use</code> during OSD create Run:<code>ceph-volume lvm list</code>Then remove with:<code>sgdisk --zap-all /dev/sdX &amp;&amp; wipefs -a /dev/sdX</code> <code>too many PGs per OSD</code> warning Reduce PGs or use:<code>ceph config set global mon_max_pg_per_osd 500</code> CephFS mount fails at boot Clean entry from <code>storage.cfg</code> or:<code>systemctl mask mnt-pve-ceph-fs.mount</code> <p>Always finish with:</p> <pre><code>ceph -s\n</code></pre> <p>Target: HEALTH_OK</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/","title":"Troubleshooting OSD Stuck / Undersized PGs","text":""},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/#problem","title":"Problem","text":"<p>You may see this:</p> <pre><code>HEALTH_WARN: Degraded data redundancy: 1 pg undersized\npg 3.3e is stuck undersized\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check if all OSDs are <code>up</code> and <code>in</code>:</li> </ol> <pre><code>ceph osd tree\nceph osd stat\n</code></pre> <ol> <li>If PG is stuck in <code>undersized+remapped</code>, use:</li> </ol> <pre><code>ceph pg repair &lt;pgid&gt;\n</code></pre> <ol> <li>Rebalance:</li> </ol> <pre><code>ceph osd reweight-by-utilization\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/#fix","title":"Fix","text":"<pre><code>ceph osd in &lt;osd.id&gt;  # if previously out\nceph pg repair &lt;pgid&gt;\n</code></pre> <p>Check:</p> <pre><code>ceph -s\n</code></pre> <p>Until you see <code>HEALTH_OK</code>.</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/","title":"Cluster Troubleshooting","text":""},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/#issue-node-already-part-of-a-cluster","title":"Issue: Node Already Part of a Cluster","text":"<p>If a node was previously part of another cluster or initialized incorrectly, reset it:</p> <pre><code>systemctl stop corosync pve-cluster\nrm -rf /etc/corosync/* /var/lib/pve-cluster/*\nsystemctl start pve-cluster\npvecm add 10.10.0.11\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/#issue-corosync-token-has-not-been-received-errors","title":"Issue: Corosync \u201cToken has not been received\u201d Errors","text":"<ul> <li>Verify VLAN 30 (cluster network) is reachable from all nodes:</li> </ul> <pre><code>ping 10.30.0.x\n</code></pre> <ul> <li>Ensure the MTU is consistent across interfaces (e.g., all at <code>1500</code> or <code>9000</code>):</li> </ul> <pre><code>ip link set dev &lt;interface&gt; mtu 1500\n</code></pre> <p>Replace <code>&lt;interface&gt;</code> with the correct NIC (e.g., <code>vmbr1</code> or <code>eth1</code>).</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/#issue-host-key-verification-failed-when-opening-console","title":"Issue: Host Key Verification Failed When Opening Console","text":"<p>This happens if certificates are mismatched. Run the following on each node:</p> <pre><code>pvecm updatecerts --force\nsystemctl restart pveproxy pvedaemon\n</code></pre> <p>This regenerates and synchronizes the cluster certificates.</p>"},{"location":"vmware/","title":"Overview","text":"<p>Work in progress</p>"}]}