{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Virtualization Repository","text":"<p>This site collects reference deployments for hypervisors and container platforms used in Vesta Lab projects.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Proxmox VE 8.4.1 \u2013 full bare-metal cluster walkthrough  </li> <li>VMware ESXi \u2013 work in progress </li> <li>Docker / Portainer \u2013 work in progress</li> <li>KVM / QEMU / Libvirt \u2013 work in progress</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/","title":"Proxmox VE 8.4.1 Documentation","text":"<p>This folder hosts the step-by-step guides for deploying and operating a bare-metal three-node Proxmox VE 8.4.1 cluster. Each page follows the Vesta Lab documentation template (Quick-view topology, Prerequisites, Step 1\u20133, Validation, Troubleshooting).</p> Page What you\u2019ll find <code>installation.md</code> ISO install, initial networking, subscription keys <code>network.md</code> Bridge/VLAN layout, bond creation, validation pings <code>cluster.md</code> Joining nodes, quorum checks, fencing hints <code>ceph.md</code> OSD prep, pool/CRUSH rules, CephFS storage screenshots <code>ha.md</code> Enable HA manager, migrate test VM, fail-over demo"},{"location":"proxmox/8.4.1/bare-metal/docs/#quick-links","title":"Quick links","text":"<ul> <li>Installation guide \u279c <code>installation.md</code> </li> <li>Networking (VLAN &amp; bridges) \u279c <code>network.md</code> </li> <li>Cluster setup \u279c <code>cluster.md</code> </li> <li>Ceph storage \u279c <code>ceph.md</code> </li> <li>High Availability \u279c <code>ha.md</code></li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Ceph Storage","text":"<p>Ceph provides scalable, fault\u2011tolerant storage for VMs and containers.  This guide walks through installing Ceph on a three\u2011node Proxmox cluster, creating OSDs and pools, and adding CephFS for ISO/template storage.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Node Cluster VLAN (10.30.0.0/24) Disks for OSDs Notes ve1 10.30.0.2 <code>/dev/sda</code> (ssd) System disk separate ve2 10.30.0.3 <code>/dev/nvme0n1</code> (ssd) \u2014 ve3 10.30.0.4 <code>/dev/sda</code> (hdd) Lower performance"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#prerequisites","title":"Prerequisites","text":"<ul> <li>A healthy Proxmox cluster with quorum (see Cluster Setup).</li> <li>At least one unused disk per node dedicated for Ceph OSDs.  All existing partitions on these disks will be destroyed.</li> <li>Sufficient network bandwidth on VLAN\u00a030 for replication traffic.  Using jumbo frames (MTU\u00a09000) is recommended.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-1-install-ceph-packages","title":"Step\u00a01\u00a0\u2013 Install Ceph Packages","text":"<p>Install Ceph on every node using the no\u2011subscription repository:</p> <pre><code>pveceph install --repository no-subscription\n</code></pre> <p>This downloads and installs the Ceph <code>quincy</code> or <code>reef</code> packages from Proxmox repositories.  Confirm success by running <code>ceph --version</code>.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-2-initialise-the-ceph-cluster","title":"Step\u00a02\u00a0\u2013 Initialise the Ceph Cluster","text":"<p>Run the initialization once on ve1, specifying the cluster network (VLAN\u00a030):</p> <pre><code>pveceph init --network 10.30.0.0/24\n</code></pre> <p>Then create the Monitor and Manager daemons on each node:</p> <pre><code>pveceph mon create\npveceph mgr create\n</code></pre> <p>In the web GUI, you will see the new monitors and managers appear under Datacenter \u2192 Ceph.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-3-create-osds","title":"Step\u00a03\u00a0\u2013 Create OSDs","text":"<p>Prepare and assign disks to Ceph.  The table above lists one disk per node; adjust as needed.  For each disk run the following commands on the appropriate node:</p> <pre><code>sgdisk --zap-all /dev/sda       # remove existing partition table\nwipefs -a /dev/sda             # erase filesystem signatures\npveceph osd create /dev/sda --crush-device-class ssd\n</code></pre> <p>Replace <code>/dev/sda</code> with the correct device (e.g. <code>/dev/nvme0n1</code> or <code>/dev/sdb</code>).  This command prepares the disk, adds it to the CRUSH map with the <code>ssd</code> or <code>hdd</code> class and starts the OSD daemon.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-4-create-pools","title":"Step\u00a04\u00a0\u2013 Create Pools","text":"<p>Create at least one pool for virtual machine disks and another for backups.  For example:</p> <pre><code>pveceph pool create ceph-pool --application rbd\nceph osd pool set ceph-pool size 2\n</code></pre> <p>The <code>size</code> parameter determines the replication level.  A value of <code>2</code> means each block is stored on two different OSDs; increase to <code>3</code> for higher durability (requires at least three OSDs).</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-5-add-cephfs-for-iso-and-templates","title":"Step\u00a05\u00a0\u2013 Add CephFS for ISO and Templates","text":"<p>To store ISO files and container templates centrally, create a CephFS volume:</p> <pre><code>pveceph fs create cephfs --data-pool cephfs_data --metadata-pool cephfs_metadata\n</code></pre> <p>Then mount the filesystem in Proxmox by adding a storage entry.  Replace the monitor IPs with your cluster nodes:</p> <pre><code>pvesm add ceph-fs cephfs-store \\\n    --monhost 10.30.0.2 10.30.0.3 10.30.0.4 \\\n    --content iso,vztmpl,backup \\\n    --mountpoint /mnt/pve/ceph-fs\n</code></pre> <p>After running the command, verify the new storage appears in the web GUI under Datacenter\u00a0\u2192 Storage.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-6-validate-the-setup","title":"Step\u00a06\u00a0\u2013 Validate the Setup","text":"<p>Check overall Ceph health and storage status:</p> <pre><code>ceph -s\npvesm status\n</code></pre> <p>The <code>ceph -s</code> output should indicate <code>HEALTH_OK</code>.  The <code>pvesm status</code> command lists the new RBD and CephFS stores as available.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#troubleshooting","title":"Troubleshooting","text":"<p>Refer to the Ceph Troubleshooting page for solutions to common warnings (e.g.\u00a0<code>401 Unauthorized</code> during installation, disks already in use or PG counts).</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#next-steps-links","title":"Next steps / Links","text":"<p>Once storage is in place, enable automated failover by following the High Availability guide.  Consult the official Proxmox Ceph documentation for detailed tuning and capacity planning [\u2714 verified https://www.virtualizationhowto.com/2022/08/proxmox-update-no-subscription-repository-configuration/].</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Cluster Setup","text":"<p>This guide explains how to create and join a Proxmox cluster.  A three\u2011node cluster ensures quorum and high availability for guest workloads.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Node Management IP Cluster IP (VLAN\u00a030) Notes ve1 10.10.0.11 10.30.0.2 Primary node; will create the cluster ve2 10.10.0.12 10.30.0.3 Joins cluster as node ID\u00a02 ve3 10.10.0.13 10.30.0.4 Joins cluster as node ID\u00a03"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>All nodes have been installed with Proxmox\u00a0VE\u00a08.4.1 and networking configured as described in the Networking guide.</li> <li><code>/etc/hosts</code> lists all node names and addresses.</li> <li>Time synchronization (<code>timedatectl</code>) is enabled on every node.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-1-initialize-the-cluster-on-the-first-node","title":"Step\u00a01\u00a0\u2013 Initialize the Cluster on the First Node","text":"<p>Log into ve1 and run the following command to create a new cluster named <code>vesta\u2011lab</code>.  The <code>--link0</code> argument specifies the management interface for Corosync communication.</p> <pre><code>pvecm create vesta-lab --link0 address=10.10.0.11,priority=1\n</code></pre> <p>You should see output indicating that the new cluster configuration has been written.  The <code>pvecm</code> tool will start Corosync and the pve\u2011cluster service on the first node.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-2-join-additional-nodes","title":"Step\u00a02\u00a0\u2013 Join Additional Nodes","text":"<p>On ve2 and ve3, join the cluster by specifying the IP address of the first node and a unique node ID.  Replace <code>&lt;nodeid&gt;</code> with <code>2</code> on ve2 and <code>3</code> on ve3:</p> <pre><code>pvecm add 10.10.0.11 --nodeid &lt;nodeid&gt;\n</code></pre> <p>If you previously initialized a local cluster on ve2 or ve3 by mistake, reset it before joining:</p> <pre><code>systemctl stop corosync pve-cluster\nrm -rf /etc/corosync/* /var/lib/pve-cluster/*\nsystemctl start pve-cluster\n</code></pre> <p>After joining, <code>pvecm nodes</code> will list all members with their assigned IDs and votes.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-3-regenerate-certificates-optional","title":"Step\u00a03\u00a0\u2013 Regenerate Certificates (Optional)","text":"<p>To ensure that all nodes have synchronized SSL certificates, run the following commands on ve1 after all members have joined:</p> <pre><code>pvecm updatecerts --force\nsystemctl restart pveproxy pvedaemon\n</code></pre> <p>This forces a certificate refresh and restarts the web proxy and API daemons.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-4-validate-cluster-status","title":"Step\u00a04\u00a0\u2013 Validate Cluster Status","text":"<p>Run <code>pvecm status</code> on any node to verify cluster health:</p> <pre><code>pvecm status\n</code></pre> <p>The output should show <code>Quorate:\u00a0Yes</code> and list all three nodes.  If quorum is not established, double\u2011check time synchronization, firewall rules and the cluster network.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#validation-smoke-tests","title":"Validation &amp; Smoke Tests","text":"<ul> <li>Node visibility \u2013 Each node appears in the web GUI under Datacenter \u2192 Cluster with status Online.</li> <li>Quorum \u2013 <code>pvecm status</code> reports <code>Quorate: Yes</code> and <code>Nodes: 3</code>.</li> <li>Migration \u2013 Create a test VM and migrate it between nodes via VM \u2192 Migration to confirm cluster operation.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#troubleshooting","title":"Troubleshooting","text":"<p>Common cluster issues (nodes stuck in old clusters, Corosync errors, certificate mismatches) are documented in the Cluster Troubleshooting page.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#next-steps-links","title":"Next steps / Links","text":"<p>With the cluster operational, continue with the Ceph Storage guide to configure shared storage across the nodes, or enable automatic failover via High Availability.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 High\u00a0Availability (HA)","text":"<p>High Availability allows critical virtual machines to restart automatically on another node if a host fails.  This guide assumes you have a three\u2011node cluster and shared storage (Ceph RBD or CephFS) configured.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#quickview-overview","title":"Quick\u2011view overview","text":"<p>In a Proxmox VE HA cluster, a Cluster Resource Manager monitors the health of nodes and starts HA\u2011enabled services on another node when a failure is detected.  Fencing (hardware watchdogs) is recommended for production but optional in a lab environment.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Proxmox cluster with at least 3 nodes and quorum (see Cluster Setup).</li> <li>Shared storage (e.g. Ceph RBD, CephFS) so VMs can run on any node.</li> <li>Time synchronization across all nodes.</li> <li>Optional: a fencing/watchdog device for reliable node isolation.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-1-verify-cluster-status","title":"Step\u00a01\u00a0\u2013 Verify Cluster Status","text":"<p>Ensure the cluster is healthy before enabling HA:</p> <pre><code>pvecm status\n</code></pre> <p>The command should show <code>Quorate: Yes</code> and all nodes listed.  Resolve any networking or quorum issues before proceeding.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-2-enable-the-ha-manager-services","title":"Step\u00a02\u00a0\u2013 Enable the HA Manager Services","text":"<p>The HA agent (<code>pve-ha-crm</code>) and local resource manager (<code>pve-ha-lrm</code>) must be running on all nodes.  Check their status and enable them if necessary:</p> <pre><code>systemctl status pve-ha-lrm\nsystemctl status pve-ha-crm\n\nsystemctl enable --now pve-ha-lrm\nsystemctl enable --now pve-ha-crm\n</code></pre> <p>These services coordinate failover actions and track which node owns each HA resource.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-3-mark-vms-or-containers-as-hamanaged","title":"Step\u00a03\u00a0\u2013 Mark VMs or Containers as HA\u2011Managed","text":"<p>Only VMs or containers flagged for HA will be automatically restarted.  You can configure HA either via the GUI or the CLI.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#cli","title":"CLI","text":"<p>Add a VM with ID\u00a0100 to HA management:</p> <pre><code>ha-manager add vm:100\n</code></pre> <p>List HA\u2011managed services:</p> <pre><code>ha-manager status\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#gui","title":"GUI","text":"<p>Navigate to Datacenter\u00a0\u2192 HA and click Add.  Select the VM or container and set the desired Group and Failover Mode.  Groups define which nodes are eligible to run the resource.  The most common mode is migrate, which starts the VM on another node.</p> Menu path Action Purpose Datacenter\u00a0\u2192 HA\u00a0\u2192 Add Choose VM/LXC ID, select group and failover settings Enable HA management for a resource Datacenter\u00a0\u2192 HA\u00a0\u2192 Groups Create groups of nodes Control where HA resources may run Datacenter\u00a0\u2192 HA\u00a0\u2192 Status View current state of HA resources Monitor failover and recovery operations <p>The screenshot below shows the HA status page with one VM registered for high availability:</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-4-test-ha-failover","title":"Step\u00a04\u00a0\u2013 Test HA Failover","text":"<p>To verify HA, simulate a node failure and watch the VM automatically start on another node.  In a lab, the simplest test is to stop the cluster stack on one node:</p> <pre><code>systemctl stop pve-cluster\n</code></pre> <p>Alternatively, reboot the node or pull its network cable (in a controlled environment).  Monitor <code>ha-manager status</code> and the web GUI to see the VM migrate and restart.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#validation-smoke-tests","title":"Validation &amp; Smoke Tests","text":"<ul> <li>Service Migration \u2013 When a node goes offline, HA\u2011enabled VMs should restart on a remaining node within a few minutes.</li> <li>Group Constraints \u2013 If a resource is restricted to a group, verify that it does not start on non\u2011group nodes.</li> <li>State Recovery \u2013 After the failed node returns, the VM continues to run on its new host.  Migration back occurs only if configured.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If HA resources remain stopped after a failure, ensure the HA services (<code>pve-ha-crm</code> and <code>pve-ha-lrm</code>) are running on all nodes.</li> <li>Use <code>ha-manager crm-log</code> to view recent cluster events and identify why a resource was not restarted.</li> <li>For production, implement STONITH/fencing devices so that failed nodes are powered off and cannot corrupt shared storage.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#next-steps-links","title":"Next steps / Links","text":"<p>HA ensures service continuity but does not replace backups.  To protect your data, integrate Proxmox Backup Server or another backup solution.  Return to Ceph Storage for advanced tuning, or explore other services in the Vesta\u00a0Lab portfolio.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Bare\u2011Metal Installation","text":""},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Component VLAN / Subnet Role Notes ve1 mgmt\u00a010.10.0.11/24 Proxmox node 1 NVMe\u00a0250\u202fGB + NVMe\u00a0480\u202fGB ve2 mgmt\u00a010.10.0.12/24 Proxmox node 2 SATA\u00a0240\u202fGB + SSD\u00a0120\u202fGB ve3 mgmt\u00a010.10.0.13/24 Proxmox node 3 NVMe\u00a0500\u202fGB + HDD\u00a01\u202fTB <p>The cluster network (VLAN\u00a030) and additional service VLANs (e.g.\u00a020 and\u00a040) will be configured later.  IPs shown here are examples; adjust them according to your environment.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Hardware \u2013 Three x86\u201164 hosts with at least 2 CPU cores and 8\u00a0GB of RAM each.  Each node should provide two storage devices: one for the OS and one for Ceph/VM data.</li> <li>Network \u2013 Management VLAN (<code>10.10.0.0/24</code>) available on all nodes.  The switch ports should be configured as trunk ports to carry all required VLANs.</li> <li>Software \u2013 Download the Proxmox\u00a0VE\u00a08.4.1 ISO from the official download page [\u2714 verified https://www.virtualizationhowto.com/2025/04/proxmox-ve-8-4-released-new-features-and-download/#proxmox-ve-8-4-download].  Prepare a bootable USB drive using tools such as Balena\u00a0Etcher.</li> <li>Access \u2013 Console or IPMI access to each node, and credentials for DNS/IP planning.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-1-install-proxmox-ve-841","title":"Step\u00a01\u00a0\u2013 Install Proxmox\u00a0VE 8.4.1","text":"<ol> <li>Boot from the ISO on each host and choose Install Proxmox\u00a0VE.</li> <li>When prompted for the target disk, select a local disk.  For lab deployments a ZFS (single disk) layout works well; for production you may choose ext4 or LVM\u2011thin.</li> <li>Assign the management IP address (VLAN\u00a010) and a hostname for each node (e.g.\u00a0<code>ve1.vestasec.com</code>).</li> <li>Set a strong root password and complete the installation wizard.</li> </ol>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-2-configure-the-apt-repositories","title":"Step\u00a02\u00a0\u2013 Configure the APT Repositories","text":"<p>After the first boot, switch to the free repository and apply all updates.  Proxmox points to the enterprise repository by default; editing the sources avoids subscription warnings:</p> <pre><code>echo \"deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription\" &gt; /etc/apt/sources.list.d/pve-no-sub.list\napt update &amp;&amp; apt full-upgrade -y\n</code></pre> <p>This configuration uses the pve\u2011no\u2011subscription repository so you can receive updates without a subscription [\u2714 verified https://www.virtualizationhowto.com/2022/08/proxmox-update-no-subscription-repository-configuration/#proxmox-8-and-higher].  If <code>/etc/apt/sources.list.d/pve-enterprise.list</code> exists, comment its single <code>deb</code> line to disable the enterprise repository:</p> <pre><code>sed -i 's/^deb/#deb/' /etc/apt/sources.list.d/pve-enterprise.list\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-3-enable-time-synchronisation","title":"Step\u00a03\u00a0\u2013 Enable Time Synchronisation","text":"<p>Consistent time is essential for clustering and Ceph.  Enable NTP on all nodes:</p> <pre><code>timedatectl set-ntp true\n</code></pre> <p>Verify the current time with <code>timedatectl status</code> and ensure all hosts show NTP\u00a0synchronized:\u00a0yes.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-4-configure-the-hosts-file","title":"Step\u00a04\u00a0\u2013 Configure the Hosts File","text":"<p>Add all nodes to <code>/etc/hosts</code> on every server so that cluster services can resolve peer names:</p> <pre><code>10.10.0.11 ve1.vestasec.com ve1\n10.10.0.12 ve2.vestasec.com ve2\n10.10.0.13 ve3.vestasec.com ve3\n</code></pre> <p>After saving the file, verify name resolution with <code>ping ve2</code> and <code>ping ve3</code> from each node.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-5-reboot-and-validate","title":"Step\u00a05\u00a0\u2013 Reboot and Validate","text":"<p>Reboot each node to apply kernel updates and network changes:</p> <pre><code>reboot\n</code></pre> <p>When all nodes are back online, log in to the web interface at <code>https://&lt;node_ip&gt;:8006/</code> and verify that you can authenticate as <code>root@pam</code>.  Use <code>ping</code> to confirm connectivity between nodes.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#validation-smoke-tests","title":"Validation &amp; Smoke Tests","text":"<ul> <li>Network Reachability \u2013 Each node should respond to pings from the others on the management network.</li> <li>Package Updates \u2013 <code>apt update</code> should succeed without subscription warnings.</li> <li>Web UI \u2013 Access the Proxmox GUI on each node and confirm the dashboard loads.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If a node cannot join the cluster or shows stale certificates later, refer to the Cluster Troubleshooting page for reset procedures.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#next-steps-links","title":"Next steps / Links","text":"<p>Proceed to the Networking guide to configure VLAN\u2011aware bridges, or jump directly to Cluster Setup once networking is complete.</p> <p>For release notes and additional documentation, consult the official Proxmox documentation [\u2714 verified https://www.proxmox.com/en/downloads].</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Networking (Bare\u2011Metal)","text":"<p>This page describes how to configure bridges and VLAN tagging on each Proxmox host.  A consistent network layout is critical for cluster communication and Ceph traffic.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"VLAN ID Name Purpose CIDR 10 mgmt Proxmox GUI / SSH 10.10.0.0/24 20 services Guest VM/LXC networks 10.20.0.0/24 30 cluster Corosync &amp; Ceph replication 10.30.0.0/24 40 backups Backup traffic 10.40.0.0/24"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ensure your physical switches trunk all required VLANs to the Proxmox hosts.  Untagged (native) VLAN\u00a010 is used for management in this example.</li> <li>Identify the network interface name connected to your switch (e.g. <code>eno1</code> or <code>eth0</code>).  You can list interfaces with <code>ip link</code>.</li> <li>Decide which VLANs will carry Ceph and cluster traffic; high\u2011throughput networks benefit from jumbo frames (MTU\u00a09000).</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#step-1-create-a-vlanaware-bridge","title":"Step\u00a01\u00a0\u2013 Create a VLAN\u2011aware Bridge","text":"<p>Edit <code>/etc/network/interfaces</code> on each node and create a Linux bridge named <code>vmbr0</code> that attaches to your physical NIC.  Enable VLAN\u2011aware mode so that sub\u2011interfaces can be defined on top:</p> <pre><code>auto lo\niface lo inet loopback\n\nauto eno1\niface eno1 inet manual\n\nauto vmbr0\niface vmbr0 inet manual\n        bridge\u2011ports eno1\n        bridge\u2011stp off\n        bridge\u2011fd 0\n        bridge\u2011vlan\u2011aware yes\n        bridge\u2011vids 2\u20114094\n</code></pre> <p>This defines a bare bridge without any IP address.  The <code>bridge\u2011vlan\u2011aware</code> flag allows us to tag VLANs on the bridge directly.  After saving, restart the networking service or reboot to apply changes.</p> <p></p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#step-2-define-vlan-subinterfaces","title":"Step\u00a02\u00a0\u2013 Define VLAN Subinterfaces","text":"<p>For each VLAN, create a logical interface on <code>vmbr0</code> with a <code>.VLAN_ID</code> suffix and assign an IP address if needed.  Below is an example for the management network.  Repeat the stanza for other VLANs, changing the VLAN ID and IP accordingly:</p> <pre><code>auto vmbr0.10\niface vmbr0.10 inet static\n        address 10.10.0.11/24\n        gateway 10.10.0.1\n        dns\u2011servers 10.0.0.102 1.1.1.1\n        vlan\u2011raw\u2011device vmbr0\n</code></pre> <p>Replace <code>10.10.0.11</code> with the correct IP for each node.  For VLANs used exclusively for Ceph or cluster traffic, omit the <code>gateway</code> line since they are non\u2011routed subnets.</p> <p>Below is a screenshot of the VLAN\u2011tagged interface configuration in the Proxmox GUI:</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#gui-configuration","title":"GUI configuration","text":"<p>If you prefer the web interface, navigate to Datacenter \u2192 Node \u2192 Network and click Create \u2192 Linux Bridge.  Enter <code>vmbr0</code> as the name, select your NIC (e.g.\u00a0<code>eno1</code>) and tick VLAN aware.  Next, create VLANs by clicking Add \u2192 Linux VLAN, choose the bridge, and enter the VLAN ID and IP settings.</p> Menu path Action Purpose Datacenter \u2192 Node \u2192 Network \u2192 Create Bridge Create <code>vmbr0</code> with VLAN awareness Base bridge for all tagged networks Datacenter \u2192 Node \u2192 Network \u2192 Add VLAN Add <code>vmbr0.10</code>, <code>vmbr0.20</code>, etc. Define per\u2011VLAN interfaces and IP addresses Datacenter \u2192 Node \u2192 Network \u2192 Reboot required Apply changes Restart services to apply network changes"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#step-3-validate-connectivity","title":"Step\u00a03\u00a0\u2013 Validate Connectivity","text":"<p>After applying the configuration, verify network connectivity:</p> <pre><code>ip addr show vmbr0\nip addr show vmbr0.10\nping -c3 10.10.0.12\nping -c3 10.30.0.3\n</code></pre> <p>All nodes should be reachable on the management and cluster VLANs.  Use <code>ip link</code> to check the MTU; set it to 9000 on both the host NIC and switch if jumbo frames are desired.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If <code>bridge\u2011vlan\u2011aware</code> is not enabled, tagged interfaces (<code>vmbr0.10</code>) will not forward traffic.  Recheck the configuration and restart networking.</li> <li>Ensure the trunk ports on the switch allow all required VLANs and that native VLANs are correctly set.</li> <li>For Ceph and Corosync, avoid mixing MTU sizes on different paths.  Use <code>ip link set dev &lt;iface&gt; mtu &lt;value&gt;</code> to adjust.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#next-steps-links","title":"Next steps / Links","text":"<p>With networking configured, proceed to the Cluster Setup page to form a three\u2011node Proxmox cluster.  For Ceph storage configuration, see the Ceph Storage guide.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_ceph/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Ceph Troubleshooting","text":"<p>Ceph is robust but occasionally returns warnings or errors during installation and operation.  Use the table below to diagnose common issues.</p> Symptom Cause Fix <code>401 Unauthorized</code> when installing Ceph The Ceph packages are pulled from the Proxmox enterprise repository, which requires a subscription. Configure the no\u2011subscription Ceph repository: <code>echo 'deb http://download.proxmox.com/debian/ceph-quincy bookworm no-subscription' &gt; /etc/apt/sources.list.d/ceph.list</code> Then run <code>apt update</code> and repeat the installation. <code>device is already in use</code> during OSD creation The target disk contains existing partitions or Ceph metadata from previous setups. List existing logical volumes with <code>ceph-volume lvm list</code>.  Remove all signatures, then recreate the OSD: <code>sgdisk --zap-all /dev/sdX &amp;&amp; wipefs -a /dev/sdX</code> Replace <code>/dev/sdX</code> with the appropriate device. \u201ctoo many PGs per OSD\u201d warning The pool has more placement groups (PGs) than recommended for the number of OSDs, leading to over\u2011utilisation. Reduce the number of PGs when creating pools or set a higher limit: <code>ceph osd pool set &lt;pool&gt; pg_num &lt;value&gt;</code> <code>ceph config set global mon_max_pg_per_osd 500</code> CephFS mount fails at boot The CephFS entry in <code>storage.cfg</code> or systemd mount unit is stale. Remove the problematic entry from <code>/etc/pve/storage.cfg</code> or mask the mount unit: <code>systemctl mask mnt-pve-ceph-fs.mount</code>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_ceph/#validation","title":"Validation","text":"<p>After applying a fix, always check the overall cluster health:</p> <pre><code>ceph -s\n</code></pre> <p>The output should indicate <code>HEALTH_OK</code>.  For pool\u2011related warnings, also run <code>ceph osd df tree</code> to inspect OSD utilisation.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_ceph/#additional-resources","title":"Additional resources","text":"<p>The Proxmox Ceph documentation contains detailed explanations of Ceph components and advanced tuning.  Refer to Ceph Storage for installation and configuration steps.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_cluster/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Cluster Troubleshooting","text":"<p>This page collects common issues encountered when forming or operating a Proxmox cluster, along with recommended fixes.  Work through the symptoms below to diagnose problems before posting to the forums.</p> Symptom Cause Fix Node is already part of another cluster The node was previously initialized or joined to a different cluster.  Corosync and cluster metadata remain on disk. Stop the cluster services, remove the Corosync and PVE cluster configuration, then restart: <code>systemctl stop corosync pve-cluster</code> <code>rm -rf /etc/corosync/* /var/lib/pve-cluster/*</code> <code>systemctl start pve-cluster</code> After cleaning, run <code>pvecm add &lt;ve1-ip&gt;</code> to join the correct cluster. Corosync log shows \u201cToken has not been received\u201d Packet loss on the cluster network or mismatched MTU settings prevents nodes from exchanging heartbeats. Verify that VLAN\u00a030 is reachable from all nodes (e.g.\u00a0<code>ping 10.30.0.3</code>).  Ensure jumbo frames are consistent across interfaces; set the MTU to 1500 or 9000 on both the NIC and switch: <code>ip link set dev vmbr0 mtu 1500</code> Host key verification failed when opening console SSL certificates across nodes are out of sync, often after rebuilding a node or restoring from backup. Regenerate cluster certificates from ve1 and restart services: <code>pvecm updatecerts --force</code> <code>systemctl restart pveproxy pvedaemon</code>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_cluster/#usage-notes","title":"Usage notes","text":"<ul> <li>Always check <code>pvecm status</code> and <code>/var/log/syslog</code> for additional clues when troubleshooting cluster issues.</li> <li>When resetting a node, ensure that the node ID you assign is not already in use in the cluster.</li> </ul> <p>Return to Cluster Setup once issues have been resolved.</p>"}]}