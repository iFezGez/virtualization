{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Virtualization Repository","text":"<p>Welcome to the Vesta Lab Virtualization stack.</p> <p>This site collects reference deployments for hypervisors and container platforms used in Vesta Lab projects.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Proxmox VE 8.4.1 \u2013 full bare-metal cluster walkthrough</li> <li>Docker 28.3.3 \u2013 deployed on Debian VM inside Proxmox</li> <li>Portainer 2.20.2 \u2013 Docker GUI installed in VMs dock1 &amp; dock2</li> </ul>"},{"location":"docker/28.3.3/vm/docs/","title":"Docker 28.3.3 on Debian 12.11","text":""},{"location":"docker/28.3.3/vm/docs/#overview","title":"Overview","text":"<p>This documentation covers the deployment and configuration of Docker 28.3.3 on a Debian 12.11 virtual machine inside Proxmox VE. All content adheres to the Vesta Lab documentation standard, including structured folders and validated command usage.</p> <p>Explore: - Installation - Configuration - Networking - Features</p>"},{"location":"docker/28.3.3/vm/docs/configuration/","title":"Docker 28.3.3 Configuration","text":""},{"location":"docker/28.3.3/vm/docs/configuration/#introduction","title":"Introduction","text":"<p>This document provides post-installation configuration steps for Docker 28.3.3 on a Debian 12.11 VM. It includes service hardening, logging setup, and enabling user access.</p>"},{"location":"docker/28.3.3/vm/docs/configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and verified</li> <li>Root or sudo access</li> <li>Internet access for package updates</li> </ul>"},{"location":"docker/28.3.3/vm/docs/configuration/#step-1-enable-and-configure-docker-daemon","title":"Step 1 \u2013 Enable and configure Docker daemon","text":"<pre><code>systemctl enable docker\nsystemctl start docker\n</code></pre> <p>Ensure Docker is enabled on boot and running.</p> <p></p>"},{"location":"docker/28.3.3/vm/docs/configuration/#step-2-allow-non-root-user-access","title":"Step 2 \u2013 Allow non-root user access","text":"<p>Add your user to the Docker group:</p> <pre><code>usermod -aG docker &lt;username&gt;\n</code></pre> <p>Log out and log back in for group changes to take effect.</p>"},{"location":"docker/28.3.3/vm/docs/configuration/#step-3-configure-docker-daemon-json","title":"Step 3 \u2013 Configure Docker daemon JSON","text":"<p>Edit or create the daemon config:</p> <pre><code>nano /etc/docker/daemon.json\n</code></pre> <p>Example:</p> <pre><code>{\n  \"log-driver\": \"journald\",\n  \"storage-driver\": \"overlay2\"\n}\n</code></pre> <p>Then restart Docker:</p> <pre><code>systemctl restart docker\n</code></pre>"},{"location":"docker/28.3.3/vm/docs/configuration/#validation-and-tests","title":"Validation and tests","text":"<pre><code>docker info\n</code></pre> <p>Check for correct storage and logging configuration.</p>"},{"location":"docker/28.3.3/vm/docs/configuration/#common-issues","title":"Common Issues","text":"Symptom Cause Fix <code>Got permission denied</code> Group change not applied Logout/login or <code>newgrp docker</code> Daemon config ignored Syntax error in JSON Validate with <code>jq .</code> or similar"},{"location":"docker/28.3.3/vm/docs/configuration/#next-steps","title":"Next steps","text":"<p>Proceed to network.md for interface bindings and firewall configuration.</p>"},{"location":"docker/28.3.3/vm/docs/features/","title":"Docker Features Overview","text":""},{"location":"docker/28.3.3/vm/docs/features/#introduction","title":"Introduction","text":"<p>This page summarizes the key Docker capabilities enabled in this environment and provides links to advanced usage topics.</p>"},{"location":"docker/28.3.3/vm/docs/features/#feature-matrix","title":"Feature Matrix","text":"Feature Enabled Notes Docker CLI \u2705 Installed with version 28.3.3 BuildKit support \u2705 Enabled by default in recent Docker Systemd startup \u2705 Integrated with <code>systemctl</code> Rootless Docker \u274c Not configured Docker Compose v2 \u2705 Installed via plugin Logging to journald \u2705 Configured via <code>daemon.json</code>"},{"location":"docker/28.3.3/vm/docs/features/#optional-tools","title":"Optional Tools","text":"<ul> <li>Portainer: UI for Docker [\u2714 verified https://www.portainer.io/]</li> <li>Docker Compose: <code>docker compose up -d</code></li> <li>Docker Registry: Self-hosted image storage</li> </ul>"},{"location":"docker/28.3.3/vm/docs/features/#next-steps","title":"Next steps","text":"<p>Review troubleshooting/ for known issues and fixes.</p>"},{"location":"docker/28.3.3/vm/docs/installation/","title":"Docker 28.3.3 Installation on Debian 12.11 (VM)","text":""},{"location":"docker/28.3.3/vm/docs/installation/#introduction","title":"Introduction","text":"<p>This guide details the complete installation process of Docker version 28.3.3 on a Debian 12.11 virtual machine deployed on Proxmox VE. It includes SSH access configuration for root and necessary post-installation tasks to ensure a clean, functional Docker environment.</p>"},{"location":"docker/28.3.3/vm/docs/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Debian 12.11 (netinst) VM installed on Proxmox VE</li> <li>Minimum 2 vCPU, 2 GB RAM, 10 GB disk</li> <li>Proxmox bridged network or VLAN assigned</li> <li>Static IP assigned or DHCP reservation</li> <li>Console or SSH access to the VM</li> <li>Verified download links:</li> <li>Debian ISO: [\u2714 verified https://www.debian.org/distrib/netinst]</li> <li>Docker install docs: [\u2714 verified https://docs.docker.com/engine/install/debian/]</li> </ul>"},{"location":"docker/28.3.3/vm/docs/installation/#step-1-enable-root-ssh-access","title":"Step 1 \u2013 Enable root SSH access","text":"<p>Edit the sshd_config file:</p> <pre><code>nano /etc/ssh/sshd_config\n</code></pre> <p>Uncomment and set:</p> <pre><code>PermitRootLogin yes\n</code></pre> <p>Then restart SSH:</p> <pre><code>systemctl restart ssh\n</code></pre> <p></p>"},{"location":"docker/28.3.3/vm/docs/installation/#step-2-install-docker-from-official-repository","title":"Step 2 \u2013 Install Docker from official repository","text":"<pre><code>apt update &amp;&amp; apt install -y ca-certificates curl gnupg\ninstall -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho   \"deb [arch=$(dpkg --print-architecture)   signed-by=/etc/apt/keyrings/docker.gpg]   https://download.docker.com/linux/debian   bookworm stable\" | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\napt update\napt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>This installs Docker 28.3.3 and its CLI components from the official source.</p>"},{"location":"docker/28.3.3/vm/docs/installation/#validation-and-tests","title":"Validation and tests","text":"<pre><code>docker --version\n</code></pre> <p>Expected output:</p> <pre><code>Docker version 28.3.3, build xxxxxxx\n</code></pre> <pre><code>docker run hello-world\n</code></pre> <p>Should print confirmation that Docker works correctly.</p>"},{"location":"docker/28.3.3/vm/docs/installation/#common-issues","title":"Common Issues","text":"Symptom Cause Fix <code>Permission denied</code> using Docker Missing group Run <code>usermod -aG docker &lt;user&gt;</code> and relogin SSH root login rejected <code>PermitRootLogin</code> still disabled Re-edit <code>/etc/ssh/sshd_config</code> and restart service"},{"location":"docker/28.3.3/vm/docs/installation/#next-steps","title":"Next steps","text":"<p>Proceed to configuration.md for Docker service tweaks and container logging options.</p>"},{"location":"docker/28.3.3/vm/docs/network/","title":"Docker Network Configuration","text":""},{"location":"docker/28.3.3/vm/docs/network/#introduction","title":"Introduction","text":"<p>This page outlines the default network behavior of Docker and how to configure bridges, external interfaces, and firewall access on Debian 12.11 in a Proxmox VM context.</p>"},{"location":"docker/28.3.3/vm/docs/network/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Component VLAN / Subnet Role Notes docker0 172.17.0.1/16 NAT bridge Default bridge created by Docker ens18 (dock1) 10.20.0.20/24 VM main iface Connected to Proxmox bridge <code>vmbr0</code> ens18 (dock2) 10.20.0.21/24 VM main iface Same subnet, separate VM vmbr0 (host) 10.20.0.1/24 Proxmox bridge Default gateway for VMs (forwarding)"},{"location":"docker/28.3.3/vm/docs/network/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and running</li> <li>Proxmox VM assigned to bridged NIC</li> <li>ufw or iptables available</li> </ul>"},{"location":"docker/28.3.3/vm/docs/network/#step-1-view-existing-docker-networks","title":"Step 1 \u2013 View existing Docker networks","text":"<pre><code>docker network ls\n</code></pre> <p>Inspect the bridge:</p> <pre><code>docker network inspect bridge\n</code></pre> <p></p>"},{"location":"docker/28.3.3/vm/docs/network/#step-2-custom-bridge-network-optional","title":"Step 2 \u2013 Custom bridge network (optional)","text":"<pre><code>docker network create --driver=bridge --subnet=10.10.0.0/24 --gateway=10.10.0.1 custom-bridge\n</code></pre>"},{"location":"docker/28.3.3/vm/docs/network/#step-3-firewall-forwarding","title":"Step 3 \u2013 Firewall forwarding","text":"<p>Ensure traffic is allowed between VM interface and Docker bridge.</p> <p>Example with ufw:</p> <pre><code>ufw allow in on docker0\nufw allow from 192.168.0.0/24 to any port 2375 proto tcp\n</code></pre>"},{"location":"docker/28.3.3/vm/docs/network/#validation-and-tests","title":"Validation and tests","text":"<ul> <li>Ping containers from host and vice versa</li> <li>Use <code>tcpdump -i docker0</code> to trace network</li> </ul>"},{"location":"docker/28.3.3/vm/docs/network/#common-issues","title":"Common Issues","text":"Symptom Cause Fix No internet from container IP forwarding disabled Enable with <code>sysctl net.ipv4.ip_forward=1</code> Cannot expose ports Host firewall blocks Check <code>ufw</code> or <code>iptables -L</code>"},{"location":"docker/28.3.3/vm/docs/network/#next-steps","title":"Next steps","text":"<p>Explore Docker Compose or Portainer via features.md.</p>"},{"location":"docker/28.3.3/vm/troubleshooting/general/","title":"Docker Troubleshooting (General)","text":""},{"location":"docker/28.3.3/vm/troubleshooting/general/#introduction","title":"Introduction","text":"<p>This document lists common issues encountered when deploying Docker 28.3.3 on Debian 12.11 (VM-based) and how to resolve them efficiently.</p>"},{"location":"docker/28.3.3/vm/troubleshooting/general/#problem-docker-not-starting","title":"Problem \u2013 Docker not starting","text":"Symptom Cause Fix <code>docker.service failed</code> Broken install or conflicting version Run <code>apt purge docker* &amp;&amp; apt install --reinstall docker-ce</code>"},{"location":"docker/28.3.3/vm/troubleshooting/general/#problem-container-has-no-internet","title":"Problem \u2013 Container has no internet","text":"Symptom Cause Fix Ping fails from container IP forwarding disabled Add <code>net.ipv4.ip_forward=1</code> to <code>/etc/sysctl.conf</code> and run <code>sysctl -p</code>"},{"location":"docker/28.3.3/vm/troubleshooting/general/#problem-cannot-pull-image-from-docker-hub","title":"Problem \u2013 Cannot pull image from Docker Hub","text":"Symptom Cause Fix TLS error / connection refused No DNS or blocked outbound ports Check <code>/etc/resolv.conf</code> or try <code>ping google.com</code>"},{"location":"docker/28.3.3/vm/troubleshooting/general/#problem-ssh-to-root-not-allowed","title":"Problem \u2013 SSH to root not allowed","text":"Symptom Cause Fix <code>Permission denied</code> when logging in <code>PermitRootLogin</code> not set Edit <code>/etc/ssh/sshd_config</code>, set <code>PermitRootLogin yes</code>, restart SSH"},{"location":"docker/28.3.3/vm/troubleshooting/general/#problem-docker-compose-not-found","title":"Problem \u2013 Docker Compose not found","text":"Symptom Cause Fix <code>docker-compose: command not found</code> Plugin not installed Make sure <code>docker-compose-plugin</code> is installed with Docker"},{"location":"docker/28.3.3/vm/troubleshooting/general/#next-steps-links","title":"Next steps / Links","text":"<ul> <li>Proceed to features.md</li> <li>Docker docs [\u2714 verified https://docs.docker.com/engine/install/debian/]</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/","title":"Portainer 2.20.2 on Debian 12.11","text":""},{"location":"portainer/2.20.2/vm/docs/#overview","title":"Overview","text":"<p>This documentation explains the deployment of Portainer 2.20.2 on Debian 12.11, hosted as a VM in Proxmox VE. It follows the standard layout used across all Vesta Lab documentation.</p> <p>Explore: - Installation - Configuration - Networking - Features</p>"},{"location":"portainer/2.20.2/vm/docs/configuration/","title":"Portainer Configuration","text":""},{"location":"portainer/2.20.2/vm/docs/configuration/#introduction","title":"Introduction","text":"<p>This document describes the initial configuration of Portainer once deployed, including admin setup, Docker environment binding, and authentication settings.</p>"},{"location":"portainer/2.20.2/vm/docs/configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker must be running on the VM</li> <li>Portainer container running and accessible (default port: <code>9443</code>)</li> <li>Firewall ports open (refer to network.md)</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/configuration/#step-1-access-portainer-web-ui","title":"Step 1 \u2013 Access Portainer Web UI","text":"<ol> <li>Open browser and visit <code>https://&lt;dock1|dock2 IP&gt;:9443</code></li> <li>Accept TLS warning (self-signed cert)</li> <li>Create admin user and password</li> </ol>"},{"location":"portainer/2.20.2/vm/docs/configuration/#step-2-bind-docker-environment","title":"Step 2 \u2013 Bind Docker environment","text":"Option Value Environment type Local URL unix:///var/run/docker.sock <p>This allows Portainer to control the local Docker engine.</p> <p></p>"},{"location":"portainer/2.20.2/vm/docs/configuration/#step-3-enable-authentication-options","title":"Step 3 \u2013 Enable Authentication Options","text":"<ul> <li>Navigate to Settings \u2192 Authentication</li> <li>Optionally enable OAuth, LDAP or SSO</li> <li>Set session timeout and password policies</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/configuration/#gui-reference-table","title":"GUI Reference Table","text":"Menu path Action Purpose Environments \u2192 Local View Docker container status Monitor local engine Settings \u2192 Authentication Configure login rules Improve access security Settings \u2192 App Templates Customize stack templates Add private templates"},{"location":"portainer/2.20.2/vm/docs/configuration/#validation-and-tests","title":"Validation and tests","text":"<ul> <li>Deploy a test container from the UI</li> <li>Navigate container logs, stats, console</li> <li>Confirm login/logout cycles and session timeout</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/configuration/#common-issues","title":"Common Issues","text":"Symptom Cause Fix UI not loading on port 9443 Port closed or service down Verify Docker logs, check firewall rules Cannot see Docker containers Missing socket mount Add <code>-v /var/run/docker.sock:/var/run/docker.sock</code> Authentication loop Browser cache Clear cookies or use private window"},{"location":"portainer/2.20.2/vm/docs/configuration/#next-steps","title":"Next steps","text":"<p>Proceed to features.md to explore container, stack and user management features.</p>"},{"location":"portainer/2.20.2/vm/docs/features/","title":"Portainer Key Features","text":""},{"location":"portainer/2.20.2/vm/docs/features/#overview","title":"Overview","text":"<p>Portainer provides a lightweight yet powerful UI to manage Docker environments. Below is a summary of its main capabilities in the current deployment.</p>"},{"location":"portainer/2.20.2/vm/docs/features/#feature-matrix","title":"Feature Matrix","text":"Feature Available Notes Container GUI \u2705 Create, start, stop, delete containers Image management \u2705 Pull, tag, delete images Volume &amp; Network \u2705 Create Docker volumes/networks Stack support \u2705 Deploy via Compose (manual upload) Registry auth \u2705 Supports DockerHub and private repos Role-based users \u2705 Only with Portainer Business Edition Resource control \ud83d\udd36 Basic; advanced needs license"},{"location":"portainer/2.20.2/vm/docs/features/#dashboard","title":"Dashboard","text":""},{"location":"portainer/2.20.2/vm/docs/features/#container-management","title":"Container Management","text":"<p>You can inspect logs, stats, and console access:</p> <p></p>"},{"location":"portainer/2.20.2/vm/docs/features/#stack-deployment","title":"Stack Deployment","text":"<p>Deploy multi-container stacks via Compose (YAML upload or text input):</p> <p></p>"},{"location":"portainer/2.20.2/vm/docs/features/#authentication-and-user-roles","title":"Authentication and User Roles","text":"<p>Configure access policies:</p> <p></p>"},{"location":"portainer/2.20.2/vm/docs/features/#recommended-usage","title":"Recommended Usage","text":"<ul> <li>Use Stacks for production deployments  </li> <li>Leverage Users &amp; Teams for shared environments  </li> <li>Mount <code>/var/run/docker.sock</code> carefully in shared setups</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/features/#next-steps","title":"Next steps","text":"<ul> <li>Consider connecting multiple endpoints (dock1 + dock2)</li> <li>Explore Portainer Agent for centralized control</li> </ul> <p>Official documentation: \u2714 verified https://docs.portainer.io</p>"},{"location":"portainer/2.20.2/vm/docs/installation/","title":"Portainer 2.20.2 Installation","text":""},{"location":"portainer/2.20.2/vm/docs/installation/#introduction","title":"Introduction","text":"<p>Step-by-step guide to install Portainer 2.20.2 CE on Debian 12.11 using Docker. Applies to VM environments within Proxmox VE.</p>"},{"location":"portainer/2.20.2/vm/docs/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Debian 12.11 installed (VM on Proxmox)</li> <li>Root SSH access enabled</li> <li>Docker 28.3.3 installed and running</li> <li>Access to port 9443 and 8000</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/installation/#step-1-create-docker-volume-for-portainer","title":"Step 1 \u2013 Create Docker volume for Portainer","text":"<pre><code>docker volume create portainer_data\n</code></pre> <p>Creates a persistent volume for Portainer data.</p>"},{"location":"portainer/2.20.2/vm/docs/installation/#step-2-deploy-portainer-container","title":"Step 2 \u2013 Deploy Portainer container","text":"<pre><code>docker run -d \\\n  -p 9443:9443 \\\n  -p 8000:8000 \\\n  --name portainer \\\n  --restart=always \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v portainer_data:/data \\\n  portainer/portainer-ce:2.20.2\n</code></pre> <p>Pulls and starts the official Portainer CE container with volume and Docker socket bind.</p> <p></p> <p>Confirm that the container is running correctly.</p>"},{"location":"portainer/2.20.2/vm/docs/installation/#step-3-access-web-ui","title":"Step 3 \u2013 Access Web UI","text":"<ol> <li>Open browser: <code>https://&lt;VM_IP&gt;:9443</code></li> <li>Accept the browser TLS warning (self-signed cert)</li> <li>Create admin password</li> <li>Choose Docker environment (Local)</li> </ol> <p>Initial admin user creation screen.</p> <p></p> <p>Select the Docker socket as your environment.</p>"},{"location":"portainer/2.20.2/vm/docs/installation/#validation-and-tests","title":"Validation and Tests","text":"<ul> <li><code>docker ps</code> shows Portainer running</li> <li>Portainer UI loads and login is successful</li> <li>Local Docker environment is visible in the dashboard</li> </ul> <p>Example of the default dashboard view.</p>"},{"location":"portainer/2.20.2/vm/docs/installation/#common-issues","title":"Common Issues","text":"Symptom Cause Fix 404 or refused connection Port not exposed Check <code>docker ps</code> and firewall rules Certificate warning Self-signed cert Proceed or replace with trusted certificate"},{"location":"portainer/2.20.2/vm/docs/installation/#next-steps","title":"Next Steps","text":"<p>See configuration.md for GUI steps and user settings.</p>"},{"location":"portainer/2.20.2/vm/docs/network/","title":"Portainer Network Configuration","text":""},{"location":"portainer/2.20.2/vm/docs/network/#introduction","title":"Introduction","text":"<p>This document describes network interfaces and required firewall rules to access Portainer deployed on Docker VMs.</p>"},{"location":"portainer/2.20.2/vm/docs/network/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Component VLAN / Subnet Role Notes ens18 (dock1) 10.20.0.20/24 VM network iface Direct access to Portainer ens18 (dock2) 10.20.0.21/24 VM network iface Secondary instance or HA option docker0 172.17.0.1/16 Docker bridge Default bridge Portainer (dock1) 9443 (HTTPS) Web UI Exposed via Docker bind"},{"location":"portainer/2.20.2/vm/docs/network/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and functional</li> <li>Host firewall open to port <code>9443</code></li> <li>SSH or web access to dock1/dock2</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/network/#step-1-verify-interface","title":"Step 1 \u2013 Verify interface","text":"<pre><code>ip a show ens18\n</code></pre> <p>Ensure it's assigned and reachable from LAN.</p>"},{"location":"portainer/2.20.2/vm/docs/network/#step-2-allow-firewall-access","title":"Step 2 \u2013 Allow firewall access","text":"<pre><code>ufw allow 9443/tcp\n</code></pre> <p>or via iptables:</p> <pre><code>iptables -A INPUT -p tcp --dport 9443 -j ACCEPT\n</code></pre>"},{"location":"portainer/2.20.2/vm/docs/network/#validation-and-tests","title":"Validation and tests","text":"<ul> <li>Access <code>https://10.20.0.20:9443</code> from browser</li> <li>Confirm HTTPS prompt and admin setup page</li> <li>Run <code>curl -k https://localhost:9443</code> locally</li> </ul>"},{"location":"portainer/2.20.2/vm/docs/network/#common-issues","title":"Common Issues","text":"Symptom Cause Fix Page not loading Port closed Check ufw/iptables Connection reset Docker not started Restart Docker service"},{"location":"portainer/2.20.2/vm/docs/network/#next-steps","title":"Next steps","text":"<p>Proceed to configuration.md to finalize UI settings and security.</p>"},{"location":"portainer/2.20.2/vm/troubleshooting/auth/","title":"Troubleshooting \u2013 Authentication","text":""},{"location":"portainer/2.20.2/vm/troubleshooting/auth/#problem-cannot-login-after-initial-setup","title":"Problem: Cannot login after initial setup","text":"Symptom Cause Fix Login fails with correct password Portainer internal DB corrupted Stop container, delete <code>portainer_data</code>, redeploy with <code>--admin-password</code>"},{"location":"portainer/2.20.2/vm/troubleshooting/auth/#problem-no-admin-password-prompt-on-fresh-install","title":"Problem: No admin password prompt on fresh install","text":"Symptom Cause Fix Setup page skipped Volume already mounted Clear or rename <code>portainer_data</code> volume"},{"location":"portainer/2.20.2/vm/troubleshooting/general/","title":"Troubleshooting \u2013 Portainer General","text":""},{"location":"portainer/2.20.2/vm/troubleshooting/general/#problem-portainer-web-ui-not-loading","title":"Problem: Portainer Web UI not loading","text":"Symptom Cause Fix Blank page or connection refused Portainer container not running Run <code>docker ps -a</code> and <code>docker start portainer</code> UI loads but errors on dashboard Backend not connected Restart Portainer and verify Docker socket mapping"},{"location":"portainer/2.20.2/vm/troubleshooting/general/#problem-cannot-deploy-stack","title":"Problem: Cannot deploy stack","text":"Symptom Cause Fix \"Permission denied\" on volume mount Incorrect host path permissions Adjust path or use <code>chown</code> to grant access"},{"location":"portainer/2.20.2/vm/troubleshooting/general/#problem-updates-not-reflected-in-container","title":"Problem: Updates not reflected in container","text":"Symptom Cause Fix Stale data after redeploy Old image used from cache Use <code>docker pull portainer/portainer-ce:2.20.2</code> before restart"},{"location":"proxmox/8.4.1/bare-metal/docs/","title":"Proxmox VE 8.4.1 on Bare-metal","text":""},{"location":"proxmox/8.4.1/bare-metal/docs/#overview","title":"Overview","text":"<p>This section documents the deployment of a three-node Proxmox VE cluster (v8.4.1) on bare-metal servers, including Ceph and HA setup.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/#contents","title":"Contents","text":"<ul> <li>Installation</li> <li>Networking</li> <li>Cluster setup</li> <li>Ceph storage</li> <li>High Availability</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Ceph Storage","text":"<p>Ceph provides scalable, fault\u2011tolerant storage for VMs and containers.  This guide walks through installing Ceph on a three\u2011node Proxmox cluster, creating OSDs and pools, and adding CephFS for ISO/template storage.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Node Cluster VLAN (10.30.0.0/24) Disks for OSDs Notes ve1 10.30.0.2 <code>/dev/sda</code> (ssd) System disk separate ve2 10.30.0.3 <code>/dev/nvme0n1</code> (ssd) \u2014 ve3 10.30.0.4 <code>/dev/sda</code> (hdd) Lower performance"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#prerequisites","title":"Prerequisites","text":"<ul> <li>A healthy Proxmox cluster with quorum (see Cluster Setup).</li> <li>At least one unused disk per node dedicated for Ceph OSDs.  All existing partitions on these disks will be destroyed.</li> <li>Sufficient network bandwidth on VLAN\u00a030 for replication traffic.  Using jumbo frames (MTU\u00a09000) is recommended.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-1-install-ceph-packages","title":"Step\u00a01\u00a0\u2013 Install Ceph Packages","text":"<p>Install Ceph on every node using the no\u2011subscription repository:</p> <pre><code>pveceph install --repository no-subscription\n</code></pre> <p>This downloads and installs the Ceph <code>quincy</code> or <code>reef</code> packages from Proxmox repositories.  Confirm success by running <code>ceph --version</code>.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-2-initialise-the-ceph-cluster","title":"Step\u00a02\u00a0\u2013 Initialise the Ceph Cluster","text":"<p>Run the initialization once on ve1, specifying the cluster network (VLAN\u00a030):</p> <pre><code>pveceph init --network 10.30.0.0/24\n</code></pre> <p>Then create the Monitor and Manager daemons on each node:</p> <pre><code>pveceph mon create\npveceph mgr create\n</code></pre> <p>In the web GUI, you will see the new monitors and managers appear under Datacenter \u2192 Ceph.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-3-create-osds","title":"Step\u00a03\u00a0\u2013 Create OSDs","text":"<p>Prepare and assign disks to Ceph.  The table above lists one disk per node; adjust as needed.  For each disk run the following commands on the appropriate node:</p> <pre><code>sgdisk --zap-all /dev/sda       # remove existing partition table\nwipefs -a /dev/sda             # erase filesystem signatures\npveceph osd create /dev/sda --crush-device-class ssd\n</code></pre> <p>Replace <code>/dev/sda</code> with the correct device (e.g. <code>/dev/nvme0n1</code> or <code>/dev/sdb</code>).  This command prepares the disk, adds it to the CRUSH map with the <code>ssd</code> or <code>hdd</code> class and starts the OSD daemon.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-4-create-pools","title":"Step\u00a04\u00a0\u2013 Create Pools","text":"<p>Create at least one pool for virtual machine disks and another for backups.  For example:</p> <pre><code>pveceph pool create ceph-pool --application rbd\nceph osd pool set ceph-pool size 2\n</code></pre> <p>The <code>size</code> parameter determines the replication level.  A value of <code>2</code> means each block is stored on two different OSDs; increase to <code>3</code> for higher durability (requires at least three OSDs).</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-5-add-cephfs-for-iso-and-templates","title":"Step\u00a05\u00a0\u2013 Add CephFS for ISO and Templates","text":"<p>To store ISO files and container templates centrally, create a CephFS volume:</p> <pre><code>pveceph fs create cephfs --data-pool cephfs_data --metadata-pool cephfs_metadata\n</code></pre> <p>Then mount the filesystem in Proxmox by adding a storage entry.  Replace the monitor IPs with your cluster nodes:</p> <pre><code>pvesm add ceph-fs cephfs-store \\\n    --monhost 10.30.0.2 10.30.0.3 10.30.0.4 \\\n    --content iso,vztmpl,backup \\\n    --mountpoint /mnt/pve/ceph-fs\n</code></pre> <p>After running the command, verify the new storage appears in the web GUI under Datacenter\u00a0\u2192 Storage.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#step-6-validate-the-setup","title":"Step\u00a06\u00a0\u2013 Validate the Setup","text":"<p>Check overall Ceph health and storage status:</p> <pre><code>ceph -s\npvesm status\n</code></pre> <p>The <code>ceph -s</code> output should indicate <code>HEALTH_OK</code>.  The <code>pvesm status</code> command lists the new RBD and CephFS stores as available.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#troubleshooting","title":"Troubleshooting","text":"<p>Refer to the Ceph Troubleshooting page for solutions to common warnings (e.g.\u00a0<code>401 Unauthorized</code> during installation, disks already in use or PG counts).</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ceph/#next-steps-links","title":"Next steps / Links","text":"<p>Once storage is in place, enable automated failover by following the High Availability guide.  Consult the official Proxmox Ceph documentation for detailed tuning and capacity planning [\u2714 verified https://www.virtualizationhowto.com/2022/08/proxmox-update-no-subscription-repository-configuration/].</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Cluster Setup","text":"<p>This guide explains how to create and join a Proxmox cluster.  A three\u2011node cluster ensures quorum and high availability for guest workloads.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Node Management IP Cluster IP (VLAN\u00a030) Notes ve1 10.10.0.11 10.30.0.2 Primary node; will create the cluster ve2 10.10.0.12 10.30.0.3 Joins cluster as node ID\u00a02 ve3 10.10.0.13 10.30.0.4 Joins cluster as node ID\u00a03"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>All nodes have been installed with Proxmox\u00a0VE\u00a08.4.1 and networking configured as described in the Networking guide.</li> <li><code>/etc/hosts</code> lists all node names and addresses.</li> <li>Time synchronization (<code>timedatectl</code>) is enabled on every node.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-1-initialize-the-cluster-on-the-first-node","title":"Step\u00a01\u00a0\u2013 Initialize the Cluster on the First Node","text":"<p>Log into ve1 and run the following command to create a new cluster named <code>vesta\u2011lab</code>.  The <code>--link0</code> argument specifies the management interface for Corosync communication.</p> <pre><code>pvecm create vesta-lab --link0 address=10.10.0.11,priority=1\n</code></pre> <p>You should see output indicating that the new cluster configuration has been written.  The <code>pvecm</code> tool will start Corosync and the pve\u2011cluster service on the first node.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-2-join-additional-nodes","title":"Step\u00a02\u00a0\u2013 Join Additional Nodes","text":"<p>On ve2 and ve3, join the cluster by specifying the IP address of the first node and a unique node ID.  Replace <code>&lt;nodeid&gt;</code> with <code>2</code> on ve2 and <code>3</code> on ve3:</p> <pre><code>pvecm add 10.10.0.11 --nodeid &lt;nodeid&gt;\n</code></pre> <p>If you previously initialized a local cluster on ve2 or ve3 by mistake, reset it before joining:</p> <pre><code>systemctl stop corosync pve-cluster\nrm -rf /etc/corosync/* /var/lib/pve-cluster/*\nsystemctl start pve-cluster\n</code></pre> <p>After joining, <code>pvecm nodes</code> will list all members with their assigned IDs and votes.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-3-regenerate-certificates-optional","title":"Step\u00a03\u00a0\u2013 Regenerate Certificates (Optional)","text":"<p>To ensure that all nodes have synchronized SSL certificates, run the following commands on ve1 after all members have joined:</p> <pre><code>pvecm updatecerts --force\nsystemctl restart pveproxy pvedaemon\n</code></pre> <p>This forces a certificate refresh and restarts the web proxy and API daemons.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#step-4-validate-cluster-status","title":"Step\u00a04\u00a0\u2013 Validate Cluster Status","text":"<p>Run <code>pvecm status</code> on any node to verify cluster health:</p> <pre><code>pvecm status\n</code></pre> <p>The output should show <code>Quorate:\u00a0Yes</code> and list all three nodes.  If quorum is not established, double\u2011check time synchronization, firewall rules and the cluster network.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#validation-smoke-tests","title":"Validation &amp; Smoke Tests","text":"<ul> <li>Node visibility \u2013 Each node appears in the web GUI under Datacenter \u2192 Cluster with status Online.</li> <li>Quorum \u2013 <code>pvecm status</code> reports <code>Quorate: Yes</code> and <code>Nodes: 3</code>.</li> <li>Migration \u2013 Create a test VM and migrate it between nodes via VM \u2192 Migration to confirm cluster operation.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#troubleshooting","title":"Troubleshooting","text":"<p>Common cluster issues (nodes stuck in old clusters, Corosync errors, certificate mismatches) are documented in the Cluster Troubleshooting page.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/cluster/#next-steps-links","title":"Next steps / Links","text":"<p>With the cluster operational, continue with the Ceph Storage guide to configure shared storage across the nodes, or enable automatic failover via High Availability.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 High\u00a0Availability (HA)","text":"<p>High Availability allows critical virtual machines to restart automatically on another node if a host fails.  This guide assumes you have a three\u2011node cluster and shared storage (Ceph RBD or CephFS) configured.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#quickview-overview","title":"Quick\u2011view overview","text":"<p>In a Proxmox VE HA cluster, a Cluster Resource Manager monitors the health of nodes and starts HA\u2011enabled services on another node when a failure is detected.  Fencing (hardware watchdogs) is recommended for production but optional in a lab environment.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Proxmox cluster with at least 3 nodes and quorum (see Cluster Setup).</li> <li>Shared storage (e.g. Ceph RBD, CephFS) so VMs can run on any node.</li> <li>Time synchronization across all nodes.</li> <li>Optional: a fencing/watchdog device for reliable node isolation.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-1-verify-cluster-status","title":"Step\u00a01\u00a0\u2013 Verify Cluster Status","text":"<p>Ensure the cluster is healthy before enabling HA:</p> <pre><code>pvecm status\n</code></pre> <p>The command should show <code>Quorate: Yes</code> and all nodes listed.  Resolve any networking or quorum issues before proceeding.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-2-enable-the-ha-manager-services","title":"Step\u00a02\u00a0\u2013 Enable the HA Manager Services","text":"<p>The HA agent (<code>pve-ha-crm</code>) and local resource manager (<code>pve-ha-lrm</code>) must be running on all nodes.  Check their status and enable them if necessary:</p> <pre><code>systemctl status pve-ha-lrm\nsystemctl status pve-ha-crm\n\nsystemctl enable --now pve-ha-lrm\nsystemctl enable --now pve-ha-crm\n</code></pre> <p>These services coordinate failover actions and track which node owns each HA resource.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-3-mark-vms-or-containers-as-hamanaged","title":"Step\u00a03\u00a0\u2013 Mark VMs or Containers as HA\u2011Managed","text":"<p>Only VMs or containers flagged for HA will be automatically restarted.  You can configure HA either via the GUI or the CLI.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#cli","title":"CLI","text":"<p>Add a VM with ID\u00a0100 to HA management:</p> <pre><code>ha-manager add vm:100\n</code></pre> <p>List HA\u2011managed services:</p> <pre><code>ha-manager status\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#gui","title":"GUI","text":"<p>Navigate to Datacenter\u00a0\u2192 HA and click Add.  Select the VM or container and set the desired Group and Failover Mode.  Groups define which nodes are eligible to run the resource.  The most common mode is migrate, which starts the VM on another node.</p> Menu path Action Purpose Datacenter\u00a0\u2192 HA\u00a0\u2192 Add Choose VM/LXC ID, select group and failover settings Enable HA management for a resource Datacenter\u00a0\u2192 HA\u00a0\u2192 Groups Create groups of nodes Control where HA resources may run Datacenter\u00a0\u2192 HA\u00a0\u2192 Status View current state of HA resources Monitor failover and recovery operations <p>The screenshot below shows the HA status page with one VM registered for high availability:</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#step-4-test-ha-failover","title":"Step\u00a04\u00a0\u2013 Test HA Failover","text":"<p>To verify HA, simulate a node failure and watch the VM automatically start on another node.  In a lab, the simplest test is to stop the cluster stack on one node:</p> <pre><code>systemctl stop pve-cluster\n</code></pre> <p>Alternatively, reboot the node or pull its network cable (in a controlled environment).  Monitor <code>ha-manager status</code> and the web GUI to see the VM migrate and restart.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#validation-smoke-tests","title":"Validation &amp; Smoke Tests","text":"<ul> <li>Service Migration \u2013 When a node goes offline, HA\u2011enabled VMs should restart on a remaining node within a few minutes.</li> <li>Group Constraints \u2013 If a resource is restricted to a group, verify that it does not start on non\u2011group nodes.</li> <li>State Recovery \u2013 After the failed node returns, the VM continues to run on its new host.  Migration back occurs only if configured.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If HA resources remain stopped after a failure, ensure the HA services (<code>pve-ha-crm</code> and <code>pve-ha-lrm</code>) are running on all nodes.</li> <li>Use <code>ha-manager crm-log</code> to view recent cluster events and identify why a resource was not restarted.</li> <li>For production, implement STONITH/fencing devices so that failed nodes are powered off and cannot corrupt shared storage.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/ha/#next-steps-links","title":"Next steps / Links","text":"<p>HA ensures service continuity but does not replace backups.  To protect your data, integrate Proxmox Backup Server or another backup solution.  Return to Ceph Storage for advanced tuning, or explore other services in the Vesta\u00a0Lab portfolio.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Bare\u2011Metal Installation","text":""},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"Component VLAN / Subnet Role Notes ve1 mgmt\u00a010.10.0.11/24 Proxmox node 1 NVMe\u00a0250\u202fGB + NVMe\u00a0480\u202fGB ve2 mgmt\u00a010.10.0.12/24 Proxmox node 2 SATA\u00a0240\u202fGB + SSD\u00a0120\u202fGB ve3 mgmt\u00a010.10.0.13/24 Proxmox node 3 NVMe\u00a0500\u202fGB + HDD\u00a01\u202fTB <p>The cluster network (VLAN\u00a030) and additional service VLANs (e.g.\u00a020 and\u00a040) will be configured later.  IPs shown here are examples; adjust them according to your environment.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Hardware \u2013 Three x86\u201164 hosts with at least 2 CPU cores and 8\u00a0GB of RAM each.  Each node should provide two storage devices: one for the OS and one for Ceph/VM data.</li> <li>Network \u2013 Management VLAN (<code>10.10.0.0/24</code>) available on all nodes.  The switch ports should be configured as trunk ports to carry all required VLANs.</li> <li>Software \u2013 Download the Proxmox\u00a0VE\u00a08.4.1 ISO from the official download page [\u2714 verified https://www.virtualizationhowto.com/2025/04/proxmox-ve-8-4-released-new-features-and-download/#proxmox-ve-8-4-download].  Prepare a bootable USB drive using tools such as Balena\u00a0Etcher.</li> <li>Access \u2013 Console or IPMI access to each node, and credentials for DNS/IP planning.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-1-install-proxmox-ve-841","title":"Step\u00a01\u00a0\u2013 Install Proxmox\u00a0VE 8.4.1","text":"<ol> <li>Boot from the ISO on each host and choose Install Proxmox\u00a0VE.</li> <li>When prompted for the target disk, select a local disk.  For lab deployments a ZFS (single disk) layout works well; for production you may choose ext4 or LVM\u2011thin.</li> <li>Assign the management IP address (VLAN\u00a010) and a hostname for each node (e.g.\u00a0<code>ve1.vestasec.com</code>).</li> <li>Set a strong root password and complete the installation wizard.</li> </ol>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-2-configure-the-apt-repositories","title":"Step\u00a02\u00a0\u2013 Configure the APT Repositories","text":"<p>After the first boot, switch to the free repository and apply all updates.  Proxmox points to the enterprise repository by default; editing the sources avoids subscription warnings:</p> <pre><code>echo \"deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription\" &gt; /etc/apt/sources.list.d/pve-no-sub.list\napt update &amp;&amp; apt full-upgrade -y\n</code></pre> <p>This configuration uses the pve\u2011no\u2011subscription repository so you can receive updates without a subscription [\u2714 verified https://www.virtualizationhowto.com/2022/08/proxmox-update-no-subscription-repository-configuration/#proxmox-8-and-higher].  If <code>/etc/apt/sources.list.d/pve-enterprise.list</code> exists, comment its single <code>deb</code> line to disable the enterprise repository:</p> <pre><code>sed -i 's/^deb/#deb/' /etc/apt/sources.list.d/pve-enterprise.list\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-3-enable-time-synchronisation","title":"Step\u00a03\u00a0\u2013 Enable Time Synchronisation","text":"<p>Consistent time is essential for clustering and Ceph.  Enable NTP on all nodes:</p> <pre><code>timedatectl set-ntp true\n</code></pre> <p>Verify the current time with <code>timedatectl status</code> and ensure all hosts show NTP\u00a0synchronized:\u00a0yes.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-4-configure-the-hosts-file","title":"Step\u00a04\u00a0\u2013 Configure the Hosts File","text":"<p>Add all nodes to <code>/etc/hosts</code> on every server so that cluster services can resolve peer names:</p> <pre><code>10.10.0.11 ve1.vestasec.com ve1\n10.10.0.12 ve2.vestasec.com ve2\n10.10.0.13 ve3.vestasec.com ve3\n</code></pre> <p>After saving the file, verify name resolution with <code>ping ve2</code> and <code>ping ve3</code> from each node.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#step-5-reboot-and-validate","title":"Step\u00a05\u00a0\u2013 Reboot and Validate","text":"<p>Reboot each node to apply kernel updates and network changes:</p> <pre><code>reboot\n</code></pre> <p>When all nodes are back online, log in to the web interface at <code>https://&lt;node_ip&gt;:8006/</code> and verify that you can authenticate as <code>root@pam</code>.  Use <code>ping</code> to confirm connectivity between nodes.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#validation-smoke-tests","title":"Validation &amp; Smoke Tests","text":"<ul> <li>Network Reachability \u2013 Each node should respond to pings from the others on the management network.</li> <li>Package Updates \u2013 <code>apt update</code> should succeed without subscription warnings.</li> <li>Web UI \u2013 Access the Proxmox GUI on each node and confirm the dashboard loads.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If a node cannot join the cluster or shows stale certificates later, refer to the Cluster Troubleshooting page for reset procedures.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/installation/#next-steps-links","title":"Next steps / Links","text":"<p>Proceed to the Networking guide to configure VLAN\u2011aware bridges, or jump directly to Cluster Setup once networking is complete.</p> <p>For release notes and additional documentation, consult the official Proxmox documentation [\u2714 verified https://www.proxmox.com/en/downloads].</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Networking (Bare\u2011Metal)","text":"<p>This page describes how to configure bridges and VLAN tagging on each Proxmox host.  A consistent network layout is critical for cluster communication and Ceph traffic.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#quickview-topology-matrix","title":"Quick\u2011view topology / matrix","text":"VLAN ID Name Purpose CIDR 10 mgmt Proxmox GUI / SSH 10.10.0.0/24 20 services Guest VM/LXC networks 10.20.0.0/24 30 cluster Corosync &amp; Ceph replication 10.30.0.0/24 40 backups Backup traffic 10.40.0.0/24"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ensure your physical switches trunk all required VLANs to the Proxmox hosts.  Untagged (native) VLAN\u00a010 is used for management in this example.</li> <li>Identify the network interface name connected to your switch (e.g. <code>eno1</code> or <code>eth0</code>).  You can list interfaces with <code>ip link</code>.</li> <li>Decide which VLANs will carry Ceph and cluster traffic; high\u2011throughput networks benefit from jumbo frames (MTU\u00a09000).</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#step-1-create-a-vlanaware-bridge","title":"Step\u00a01\u00a0\u2013 Create a VLAN\u2011aware Bridge","text":"<p>Edit <code>/etc/network/interfaces</code> on each node and create a Linux bridge named <code>vmbr0</code> that attaches to your physical NIC.  Enable VLAN\u2011aware mode so that sub\u2011interfaces can be defined on top:</p> <pre><code>auto lo\niface lo inet loopback\n\nauto eno1\niface eno1 inet manual\n\nauto vmbr0\niface vmbr0 inet manual\n        bridge\u2011ports eno1\n        bridge\u2011stp off\n        bridge\u2011fd 0\n        bridge\u2011vlan\u2011aware yes\n        bridge\u2011vids 2\u20114094\n</code></pre> <p>This defines a bare bridge without any IP address.  The <code>bridge\u2011vlan\u2011aware</code> flag allows us to tag VLANs on the bridge directly.  After saving, restart the networking service or reboot to apply changes.</p> <p></p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#step-2-define-vlan-subinterfaces","title":"Step\u00a02\u00a0\u2013 Define VLAN Subinterfaces","text":"<p>For each VLAN, create a logical interface on <code>vmbr0</code> with a <code>.VLAN_ID</code> suffix and assign an IP address if needed.  Below is an example for the management network.  Repeat the stanza for other VLANs, changing the VLAN ID and IP accordingly:</p> <pre><code>auto vmbr0.10\niface vmbr0.10 inet static\n        address 10.10.0.11/24\n        gateway 10.10.0.1\n        dns\u2011servers 10.0.0.102 1.1.1.1\n        vlan\u2011raw\u2011device vmbr0\n</code></pre> <p>Replace <code>10.10.0.11</code> with the correct IP for each node.  For VLANs used exclusively for Ceph or cluster traffic, omit the <code>gateway</code> line since they are non\u2011routed subnets.</p> <p>Below is a screenshot of the VLAN\u2011tagged interface configuration in the Proxmox GUI:</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#gui-configuration","title":"GUI configuration","text":"<p>If you prefer the web interface, navigate to Datacenter \u2192 Node \u2192 Network and click Create \u2192 Linux Bridge.  Enter <code>vmbr0</code> as the name, select your NIC (e.g.\u00a0<code>eno1</code>) and tick VLAN aware.  Next, create VLANs by clicking Add \u2192 Linux VLAN, choose the bridge, and enter the VLAN ID and IP settings.</p> Menu path Action Purpose Datacenter \u2192 Node \u2192 Network \u2192 Create Bridge Create <code>vmbr0</code> with VLAN awareness Base bridge for all tagged networks Datacenter \u2192 Node \u2192 Network \u2192 Add VLAN Add <code>vmbr0.10</code>, <code>vmbr0.20</code>, etc. Define per\u2011VLAN interfaces and IP addresses Datacenter \u2192 Node \u2192 Network \u2192 Reboot required Apply changes Restart services to apply network changes"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#step-3-validate-connectivity","title":"Step\u00a03\u00a0\u2013 Validate Connectivity","text":"<p>After applying the configuration, verify network connectivity:</p> <pre><code>ip addr show vmbr0\nip addr show vmbr0.10\nping -c3 10.10.0.12\nping -c3 10.30.0.3\n</code></pre> <p>All nodes should be reachable on the management and cluster VLANs.  Use <code>ip link</code> to check the MTU; set it to 9000 on both the host NIC and switch if jumbo frames are desired.</p> <p></p>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If <code>bridge\u2011vlan\u2011aware</code> is not enabled, tagged interfaces (<code>vmbr0.10</code>) will not forward traffic.  Recheck the configuration and restart networking.</li> <li>Ensure the trunk ports on the switch allow all required VLANs and that native VLANs are correctly set.</li> <li>For Ceph and Corosync, avoid mixing MTU sizes on different paths.  Use <code>ip link set dev &lt;iface&gt; mtu &lt;value&gt;</code> to adjust.</li> </ul>"},{"location":"proxmox/8.4.1/bare-metal/docs/network/#next-steps-links","title":"Next steps / Links","text":"<p>With networking configured, proceed to the Cluster Setup page to form a three\u2011node Proxmox cluster.  For Ceph storage configuration, see the Ceph Storage guide.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_ceph/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Ceph Troubleshooting","text":"<p>Ceph is robust but occasionally returns warnings or errors during installation and operation.  Use the table below to diagnose common issues.</p> Symptom Cause Fix <code>401 Unauthorized</code> when installing Ceph The Ceph packages are pulled from the Proxmox enterprise repository, which requires a subscription. Configure the no\u2011subscription Ceph repository: <code>echo 'deb http://download.proxmox.com/debian/ceph-quincy bookworm no-subscription' &gt; /etc/apt/sources.list.d/ceph.list</code> Then run <code>apt update</code> and repeat the installation. <code>device is already in use</code> during OSD creation The target disk contains existing partitions or Ceph metadata from previous setups. List existing logical volumes with <code>ceph-volume lvm list</code>.  Remove all signatures, then recreate the OSD: <code>sgdisk --zap-all /dev/sdX &amp;&amp; wipefs -a /dev/sdX</code> Replace <code>/dev/sdX</code> with the appropriate device. \u201ctoo many PGs per OSD\u201d warning The pool has more placement groups (PGs) than recommended for the number of OSDs, leading to over\u2011utilisation. Reduce the number of PGs when creating pools or set a higher limit: <code>ceph osd pool set &lt;pool&gt; pg_num &lt;value&gt;</code> <code>ceph config set global mon_max_pg_per_osd 500</code> CephFS mount fails at boot The CephFS entry in <code>storage.cfg</code> or systemd mount unit is stale. Remove the problematic entry from <code>/etc/pve/storage.cfg</code> or mask the mount unit: <code>systemctl mask mnt-pve-ceph-fs.mount</code>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_ceph/#validation","title":"Validation","text":"<p>After applying a fix, always check the overall cluster health:</p> <pre><code>ceph -s\n</code></pre> <p>The output should indicate <code>HEALTH_OK</code>.  For pool\u2011related warnings, also run <code>ceph osd df tree</code> to inspect OSD utilisation.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_ceph/#additional-resources","title":"Additional resources","text":"<p>The Proxmox Ceph documentation contains detailed explanations of Ceph components and advanced tuning.  Refer to Ceph Storage for installation and configuration steps.</p>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_cluster/","title":"Proxmox\u00a0VE\u00a08.4.1 \u2013 Cluster Troubleshooting","text":"<p>This page collects common issues encountered when forming or operating a Proxmox cluster, along with recommended fixes.  Work through the symptoms below to diagnose problems before posting to the forums.</p> Symptom Cause Fix Node is already part of another cluster The node was previously initialized or joined to a different cluster.  Corosync and cluster metadata remain on disk. Stop the cluster services, remove the Corosync and PVE cluster configuration, then restart: <code>systemctl stop corosync pve-cluster</code> <code>rm -rf /etc/corosync/* /var/lib/pve-cluster/*</code> <code>systemctl start pve-cluster</code> After cleaning, run <code>pvecm add &lt;ve1-ip&gt;</code> to join the correct cluster. Corosync log shows \u201cToken has not been received\u201d Packet loss on the cluster network or mismatched MTU settings prevents nodes from exchanging heartbeats. Verify that VLAN\u00a030 is reachable from all nodes (e.g.\u00a0<code>ping 10.30.0.3</code>).  Ensure jumbo frames are consistent across interfaces; set the MTU to 1500 or 9000 on both the NIC and switch: <code>ip link set dev vmbr0 mtu 1500</code> Host key verification failed when opening console SSL certificates across nodes are out of sync, often after rebuilding a node or restoring from backup. Regenerate cluster certificates from ve1 and restart services: <code>pvecm updatecerts --force</code> <code>systemctl restart pveproxy pvedaemon</code>"},{"location":"proxmox/8.4.1/bare-metal/docs/troubleshooting_cluster/#usage-notes","title":"Usage notes","text":"<ul> <li>Always check <code>pvecm status</code> and <code>/var/log/syslog</code> for additional clues when troubleshooting cluster issues.</li> <li>When resetting a node, ensure that the node ID you assign is not already in use in the cluster.</li> </ul> <p>Return to Cluster Setup once issues have been resolved.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/","title":"Creating Cloud-Init VM Templates in Proxmox","text":"<p>This guide explains how to create a reusable Debian 12 template with cloud-init support in Proxmox.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#steps","title":"Steps","text":"<ol> <li>Download the Debian 12 generic cloud image (QCOW2).</li> <li>Create a new VM with ID 9000 and attach the downloaded disk.</li> <li>Configure boot settings and add a cloud-init drive.</li> <li>Mark the VM as a template.</li> </ol>"},{"location":"proxmox/8.4.1/bare-metal/templates/#script","title":"Script","text":"<p>The <code>create_debian12_template.sh</code> script automates this process:</p> <pre><code>./scripts/create_debian12_template.sh\n</code></pre> <p>Ensure the script has executable permissions and is run from the <code>proxmox/</code> directory.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#output","title":"Output","text":"<p>Once executed, you\u2019ll have a reusable cloud-init template with VM ID 9000 stored in your local Proxmox node.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#creating-cloud-init-vm-templates-in-proxmox_1","title":"Creating Cloud-Init VM Templates in Proxmox","text":"<p>This guide explains how to create a reusable Debian 12 template with cloud-init support in Proxmox.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#steps_1","title":"Steps","text":"<ol> <li>Download the Debian 12 generic cloud image (QCOW2).</li> <li>Create a new VM with ID 9000 and attach the downloaded disk.</li> <li>Configure boot settings and add a cloud-init drive.</li> <li>Mark the VM as a template.</li> </ol>"},{"location":"proxmox/8.4.1/bare-metal/templates/#script_1","title":"Script","text":"<p>The <code>create_debian12_template.sh</code> script automates this process:</p> <pre><code>./scripts/create_debian12_template.sh\n</code></pre> <p>Ensure the script has executable permissions and is run from the <code>proxmox/</code> directory.</p>"},{"location":"proxmox/8.4.1/bare-metal/templates/#output_1","title":"Output","text":"<p>Once executed, you\u2019ll have a reusable cloud-init template with VM ID 9000 stored in your local Proxmox node.</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph/","title":"Ceph Troubleshooting","text":"Symptom Fix <code>401 Unauthorized</code> when installing Ceph Use the no-subscription repo:<code>echo 'deb http://download.proxmox.com/debian/ceph-quincy bookworm no-subscription' &gt; /etc/apt/sources.list.d/ceph.list</code> <code>device is already in use</code> during OSD create Run:<code>ceph-volume lvm list</code>Then remove with:<code>sgdisk --zap-all /dev/sdX &amp;&amp; wipefs -a /dev/sdX</code> <code>too many PGs per OSD</code> warning Reduce PGs or use:<code>ceph config set global mon_max_pg_per_osd 500</code> CephFS mount fails at boot Clean entry from <code>storage.cfg</code> or:<code>systemctl mask mnt-pve-ceph-fs.mount</code> <p>Always finish with:</p> <pre><code>ceph -s\n</code></pre> <p>Target: HEALTH_OK</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/","title":"Troubleshooting OSD Stuck / Undersized PGs","text":""},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/#problem","title":"Problem","text":"<p>You may see this:</p> <pre><code>HEALTH_WARN: Degraded data redundancy: 1 pg undersized\npg 3.3e is stuck undersized\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check if all OSDs are <code>up</code> and <code>in</code>:</li> </ol> <pre><code>ceph osd tree\nceph osd stat\n</code></pre> <ol> <li>If PG is stuck in <code>undersized+remapped</code>, use:</li> </ol> <pre><code>ceph pg repair &lt;pgid&gt;\n</code></pre> <ol> <li>Rebalance:</li> </ol> <pre><code>ceph osd reweight-by-utilization\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/ceph_osd_recovery/#fix","title":"Fix","text":"<pre><code>ceph osd in &lt;osd.id&gt;  # if previously out\nceph pg repair &lt;pgid&gt;\n</code></pre> <p>Check:</p> <pre><code>ceph -s\n</code></pre> <p>Until you see <code>HEALTH_OK</code>.</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/","title":"Cluster Troubleshooting","text":""},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/#issue-node-already-part-of-a-cluster","title":"Issue: Node Already Part of a Cluster","text":"<p>If a node was previously part of another cluster or initialized incorrectly, reset it:</p> <pre><code>systemctl stop corosync pve-cluster\nrm -rf /etc/corosync/* /var/lib/pve-cluster/*\nsystemctl start pve-cluster\npvecm add 10.10.0.11\n</code></pre>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/#issue-corosync-token-has-not-been-received-errors","title":"Issue: Corosync \u201cToken has not been received\u201d Errors","text":"<ul> <li>Verify VLAN 30 (cluster network) is reachable from all nodes:</li> </ul> <pre><code>ping 10.30.0.x\n</code></pre> <ul> <li>Ensure the MTU is consistent across interfaces (e.g., all at <code>1500</code> or <code>9000</code>):</li> </ul> <pre><code>ip link set dev &lt;interface&gt; mtu 1500\n</code></pre> <p>Replace <code>&lt;interface&gt;</code> with the correct NIC (e.g., <code>vmbr1</code> or <code>eth1</code>).</p>"},{"location":"proxmox/8.4.1/bare-metal/troubleshooting/cluster/#issue-host-key-verification-failed-when-opening-console","title":"Issue: Host Key Verification Failed When Opening Console","text":"<p>This happens if certificates are mismatched. Run the following on each node:</p> <pre><code>pvecm updatecerts --force\nsystemctl restart pveproxy pvedaemon\n</code></pre> <p>This regenerates and synchronizes the cluster certificates.</p>"}]}